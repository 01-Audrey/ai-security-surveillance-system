"""
==================================================
ML LEARNING JOURNEY - DAY 24
==================================================
Week: 4 of 24
Day: 24 of 168
Date: November 19, 2025
Topic: Tracking Optimization & People Counting
Overall Progress: 14.3%

Week 4: Detection & Tracking Foundation
‚úÖ Day 22: Project Planning & Architecture (COMPLETED)
‚úÖ Day 23: Multi-Object Tracking (DeepSORT) (COMPLETED)
üîÑ Day 24: Tracking Optimization (TODAY!)
‚¨ú Day 25: Video Processing Pipeline
‚¨ú Day 26: Testing & Performance
‚¨ú Day 27: Code Cleanup & Modularization
‚¨ú Day 28: Week 4 Review

Progress: 43% (3/7 days)

==================================================
üéØ Week 4 Project: Security System - Detection & Tracking
- Optimize tracking parameters for security scenarios
- Implement people counting (entry/exit)
- Handle occlusions and re-identification
- Define zones for monitoring
- Achieve consistent 30 FPS performance

üéØ Today's Learning Objectives:
1. Tune DeepSORT parameters (max_age, n_init, IOU threshold)
2. Implement line-crossing detection for people counting
3. Handle occlusions robustly (maintain IDs through disappearance)
4. Create zone-based monitoring (restricted areas)
5. Count people entering and exiting specific areas
6. Optimize for different scenarios (crowded, sparse, occlusions)
7. Measure tracking accuracy and ID consistency

üìö Today's Structure:
   Part 1 (2h): Parameter Tuning & Occlusion Handling
   Part 2 (2h): People Counting System
   Part 3 (2h): Zone-Based Monitoring
   Part 4 (1h): Testing & Summary

üéØ SUCCESS CRITERIA:
   ‚úÖ Tracking parameters optimized (tested scenarios)
   ‚úÖ Occlusions handled (IDs maintained through disappearance)
   ‚úÖ Line-crossing detection working (entry/exit counting)
   ‚úÖ People counting accurate (¬±5% error)
   ‚úÖ Zone monitoring implemented (restricted areas)
   ‚úÖ Direction detection working (in vs out)
   ‚úÖ Performance maintained (25-30 FPS)
   ‚úÖ Ready for video processing pipeline (Day 25)
==================================================
"""


# ==================================================
# INSTALL REQUIRED LIBRARIES
# ==================================================

import subprocess
import sys

print("üì¶ Installing required libraries...")
print("‚è±Ô∏è  This should be quick (most already installed)...\n")

packages = [
    'ultralytics',
    'deep-sort-realtime',
    'opencv-python',
    'numpy',
    'pandas',
    'matplotlib',
    'scipy'
]

for package in packages:
    print(f"Checking {package}...")
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])

print("\n‚úÖ All libraries ready!")

print("\n" + "=" * 80)

# ==================================================
# IMPORT LIBRARIES
# ==================================================

print("\n" + "=" * 80)
print("üìö IMPORTING LIBRARIES")
print("=" * 80)

# Standard libraries
import os
import time
from pathlib import Path
from collections import defaultdict, deque
import json

# Data science
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial import distance

# Computer vision
import cv2

# Deep learning
from ultralytics import YOLO

# Tracking
from deep_sort_realtime.deepsort_tracker import DeepSort

print("\n‚úÖ All libraries imported successfully!")
print("\nüìä Library versions:")
print(f"   ‚Ä¢ OpenCV: {cv2.__version__}")
print(f"   ‚Ä¢ NumPy: {np.__version__}")
print(f"   ‚Ä¢ Pandas: {pd.__version__}")
print("   ‚Ä¢ Ultralytics: Installed ‚úì")
print("   ‚Ä¢ DeepSORT: Installed ‚úì")

print("=" * 80)


print("\n" + "=" * 80)
print("üìö PART 1: PARAMETER TUNING & OCCLUSION HANDLING")
print("=" * 80)


# ==================================================
# EXERCISE 1.1: UNDERSTAND DEEPSORT PARAMETERS
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 1.1: Understanding DeepSORT Parameters")
print("=" * 80)

"""
üìñ THEORY: DeepSORT Critical Parameters

The three most important parameters for tuning:

1. max_age (Default: 30)
   ‚Ä¢ How many frames to keep a track without detection
   ‚Ä¢ Higher = More robust to occlusions (but more false positives)
   ‚Ä¢ Lower = Faster deletion (but IDs lost easily)
   ‚Ä¢ Formula: max_age = expected_occlusion_time * FPS
   ‚Ä¢ Example: 1 second occlusion @ 30fps ‚Üí max_age=30

2. n_init (Default: 3)
   ‚Ä¢ Consecutive detections needed to confirm a track
   ‚Ä¢ Higher = Fewer false tracks (but slower confirmation)
   ‚Ä¢ Lower = Faster confirmation (but more false positives)
   ‚Ä¢ Typically: 2-5 frames

3. max_iou_distance (Default: 0.7)
   ‚Ä¢ Threshold for matching based on bounding box overlap
   ‚Ä¢ IOU = Intersection over Union
   ‚Ä¢ Lower = Stricter matching (fewer ID switches)
   ‚Ä¢ Higher = More lenient (more matches, potential errors)
   ‚Ä¢ Range: 0.5-0.9

Additional Parameters:

4. max_cosine_distance (Default: 0.2)
   ‚Ä¢ Threshold for appearance matching
   ‚Ä¢ Based on ReID embedding similarity
   ‚Ä¢ Lower = Objects must look very similar
   ‚Ä¢ Higher = More lenient appearance matching

5. nn_budget (Default: 100)
   ‚Ä¢ Number of appearance features to keep per track
   ‚Ä¢ Higher = Better re-identification (but more memory)
   ‚Ä¢ Lower = Less memory (but worse re-ID)

==================================================

PARAMETER TUNING GUIDELINES:

Scenario: Crowded Areas (Mall, Station)
‚îú‚îÄ‚îÄ max_age: 20-30 (frequent occlusions)
‚îú‚îÄ‚îÄ n_init: 3-4 (reduce false positives)
‚îú‚îÄ‚îÄ max_iou_distance: 0.6-0.7 (strict matching)
‚îî‚îÄ‚îÄ Goal: Reduce ID switches in crowds

Scenario: Sparse Areas (Office Hallway)
‚îú‚îÄ‚îÄ max_age: 15-20 (people rarely hidden)
‚îú‚îÄ‚îÄ n_init: 2-3 (fast confirmation)
‚îú‚îÄ‚îÄ max_iou_distance: 0.7-0.8 (lenient)
‚îî‚îÄ‚îÄ Goal: Quick tracking, smooth experience

Scenario: High Occlusion (Factory, Warehouse)
‚îú‚îÄ‚îÄ max_age: 40-60 (objects frequently hidden)
‚îú‚îÄ‚îÄ n_init: 4-5 (confirm before showing ID)
‚îú‚îÄ‚îÄ max_iou_distance: 0.5-0.6 (very strict)
‚îî‚îÄ‚îÄ Goal: Maintain IDs through long occlusions

Scenario: Fast Moving Objects (Entrance/Exit)
‚îú‚îÄ‚îÄ max_age: 10-15 (people pass quickly)
‚îú‚îÄ‚îÄ n_init: 2 (immediate confirmation)
‚îú‚îÄ‚îÄ max_iou_distance: 0.7-0.8 (allow fast motion)
‚îî‚îÄ‚îÄ Goal: Count people before they leave frame

==================================================

IMPACT ON PERFORMANCE:

Speed Impact:
- max_age: Minimal (just checks counter)
- n_init: Minimal (just checks counter)
- max_iou_distance: None (threshold only)
- nn_budget: High budget = more memory, slightly slower
- Overall: Parameter tuning doesn't significantly affect FPS

Accuracy Impact:
- max_age too low ‚Üí Lost tracks frequently
- max_age too high ‚Üí False tracks persist
- n_init too low ‚Üí Many false positives
- n_init too high ‚Üí Slow to confirm real tracks
- max_iou_distance too low ‚Üí Missed matches, ID switches
- max_iou_distance too high ‚Üí Wrong matches

==================================================

TESTING METHODOLOGY:

1. Start with defaults (max_age=30, n_init=3, iou=0.7)
2. Record baseline metrics (ID switches, lost tracks)
3. Adjust ONE parameter at a time
4. Test on representative videos
5. Measure improvement
6. Iterate until optimal

Metrics to Track:
- ID switches per minute
- Average track length
- False positive tracks
- Lost tracks (premature deletion)
- Re-identification success rate
"""

print("""
üìä PARAMETER COMPARISON TABLE:

Parameter        | Default | Crowded | Sparse  | High Occlusion | Fast Moving
-----------------|---------|---------|---------|----------------|------------
max_age          |   30    |  20-30  |  15-20  |     40-60      |   10-15
n_init           |    3    |   3-4   |   2-3   |      4-5       |     2
max_iou_distance |  0.7    | 0.6-0.7 | 0.7-0.8 |    0.5-0.6     |  0.7-0.8
max_cosine_dist  |  0.2    |  0.15   |  0.25   |      0.15      |    0.25
nn_budget        |  100    |   100   |   50    |      150       |    50

Recommendation for Security System (Office/Factory):
- Start with: max_age=30, n_init=3, max_iou_distance=0.7
- Tune based on specific environment
- Test with real footage from deployment location
""")

print("\n‚úÖ Exercise 1.1 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 1.2: CREATE PARAMETER TESTING FRAMEWORK
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 1.2: Create Parameter Testing Framework")
print("=" * 80)

"""
üìñ THEORY: Systematic Parameter Testing

Why We Need a Framework:
- Consistent testing across parameter values
- Objective performance comparison
- Track metrics over time
- Make data-driven decisions

Testing Approach:
1. Define test video/scenario
2. Create multiple tracker configurations
3. Run each configuration
4. Collect metrics
5. Compare and select best

Metrics to Collect:
- Track count (total unique IDs seen)
- Average track duration (frames)
- ID switches (when same person gets new ID)
- FPS (frames per second)
- Lost tracks (tracks deleted prematurely)
"""

class TrackerConfig:
    """Configuration for DeepSORT tracker"""
    
    def __init__(self, name, max_age=30, n_init=3, max_iou_distance=0.7):
        self.name = name
        self.max_age = max_age
        self.n_init = n_init
        self.max_iou_distance = max_iou_distance
    
    def create_tracker(self):
        """Create DeepSORT tracker with this config"""
        return DeepSort(
            max_age=self.max_age,
            n_init=self.n_init,
            max_iou_distance=self.max_iou_distance,
            embedder="mobilenet",
            embedder_gpu=False
        )
    
    def __repr__(self):
        return f"{self.name}: max_age={self.max_age}, n_init={self.n_init}, iou={self.max_iou_distance}"

print("‚úÖ Class created: TrackerConfig")
print("   ‚Ä¢ Purpose: Store and manage tracker configurations")
print("   ‚Ä¢ Methods: create_tracker(), __repr__()")

# Define test configurations
configs = [
    TrackerConfig("Default", max_age=30, n_init=3, max_iou_distance=0.7),
    TrackerConfig("High_Occlusion", max_age=50, n_init=4, max_iou_distance=0.6),
    TrackerConfig("Fast_Confirmation", max_age=20, n_init=2, max_iou_distance=0.75),
    TrackerConfig("Strict_Matching", max_age=30, n_init=3, max_iou_distance=0.5),
    TrackerConfig("Lenient_Matching", max_age=30, n_init=3, max_iou_distance=0.85),
]

print("\nüìã Test Configurations Created:")
print("=" * 80)
for i, config in enumerate(configs, 1):
    print(f"   {i}. {config}")

print("\nüí° Testing Strategy:")
print("   ‚Ä¢ Run each configuration on same video")
print("   ‚Ä¢ Collect metrics for comparison")
print("   ‚Ä¢ Select best for security system use case")

print("\n‚úÖ Exercise 1.2 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 1.3: OCCLUSION HANDLING TEST
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 1.3: Test Occlusion Handling")
print("=" * 80)

"""
üìñ THEORY: Handling Occlusions in Tracking

What is Occlusion?
- When tracked object is temporarily hidden
- Person walks behind pillar
- Person temporarily leaves frame
- Person overlapped by another person

Why Occlusions are Challenging:
- No detection = no update to Kalman filter
- Predicted position becomes less accurate
- Risk of losing track ID
- Risk of assigning new ID when reappears

How DeepSORT Handles Occlusions:

1. Kalman Filter Prediction:
   ‚Ä¢ Continues predicting position during occlusion
   ‚Ä¢ Uses last known velocity
   ‚Ä¢ Prediction uncertainty increases over time

2. Track State Management:
   ‚Ä¢ Tracked ‚Üí Lost (when no detection)
   ‚Ä¢ Lost track kept for max_age frames
   ‚Ä¢ If reappears: match by appearance + predicted position
   ‚Ä¢ If doesn't reappear: delete track

3. Appearance Descriptor:
   ‚Ä¢ ReID embedding stored for each track
   ‚Ä¢ When object reappears, match by appearance
   ‚Ä¢ Even if position prediction is off
   ‚Ä¢ Enables re-identification

Success Factors:
‚úì max_age long enough for expected occlusion
‚úì Appearance features distinctive enough
‚úì Predicted position reasonably accurate
‚úì No similar-looking objects in scene

==================================================

OCCLUSION SCENARIOS:

Short Occlusion (1-10 frames, <0.5 seconds):
‚îú‚îÄ‚îÄ Challenge: Low
‚îú‚îÄ‚îÄ Solution: Kalman prediction usually sufficient
‚îú‚îÄ‚îÄ max_age: 15-20 frames adequate
‚îî‚îÄ‚îÄ Success Rate: 95%+

Medium Occlusion (10-30 frames, 0.5-1 second):
‚îú‚îÄ‚îÄ Challenge: Moderate
‚îú‚îÄ‚îÄ Solution: Appearance matching critical
‚îú‚îÄ‚îÄ max_age: 30-40 frames needed
‚îî‚îÄ‚îÄ Success Rate: 80-90%

Long Occlusion (30+ frames, 1+ seconds):
‚îú‚îÄ‚îÄ Challenge: High
‚îú‚îÄ‚îÄ Solution: May require higher max_age + good appearance
‚îú‚îÄ‚îÄ max_age: 50-60+ frames
‚îî‚îÄ‚îÄ Success Rate: 60-80%

Permanent Occlusion (object leaves):
‚îú‚îÄ‚îÄ Challenge: N/A (expected behavior)
‚îú‚îÄ‚îÄ Solution: Track should be deleted after max_age
‚îú‚îÄ‚îÄ max_age: Balance between persistence and deletion
‚îî‚îÄ‚îÄ Goal: Delete cleanly, no ghost tracks
"""

def simulate_occlusion_scenario():
    """
    Simulate tracking through occlusion
    
    Demonstrates how max_age affects track persistence
    """
    print("\nüß™ OCCLUSION SIMULATION:")
    print("=" * 80)
    
    print("\nScenario: Person walks behind pillar")
    print("   ‚Ä¢ Visible: Frames 1-10")
    print("   ‚Ä¢ Occluded: Frames 11-25 (15 frames)")
    print("   ‚Ä¢ Reappears: Frame 26+")
    
    print("\nüìä Testing with different max_age values:")
    
    test_configs = [
        ("max_age=10", 10),
        ("max_age=20", 20),
        ("max_age=30", 30),
        ("max_age=50", 50),
    ]
    
    occlusion_duration = 15  # frames
    
    for name, max_age in test_configs:
        if max_age >= occlusion_duration:
            result = "‚úÖ Track MAINTAINED"
            explanation = f"(max_age={max_age} > occlusion={occlusion_duration})"
        else:
            result = "‚ùå Track LOST"
            explanation = f"(max_age={max_age} < occlusion={occlusion_duration})"
        
        print(f"   {name:15} ‚Üí {result:20} {explanation}")
    
    print("\nüí° Key Insight:")
    print("   ‚Ä¢ max_age must exceed expected occlusion duration")
    print("   ‚Ä¢ Too low: Frequent ID losses")
    print("   ‚Ä¢ Too high: Ghost tracks persist longer")
    print("   ‚Ä¢ Sweet spot: 1.5-2x typical occlusion duration")
    
    print("\nüìà Recommended max_age by Scenario:")
    print("   ‚Ä¢ Office (rare occlusions): 20-30 frames")
    print("   ‚Ä¢ Retail (moderate): 30-40 frames")
    print("   ‚Ä¢ Factory (frequent): 40-60 frames")
    print("   ‚Ä¢ Outdoor (variable): 30-50 frames")

simulate_occlusion_scenario()

print("\n‚úÖ Exercise 1.3 Complete!")
print("=" * 80)


print("\n" + "=" * 80)
print("üî¢ PART 2: PEOPLE COUNTING SYSTEM")
print("=" * 80)


# ==================================================
# EXERCISE 2.1: UNDERSTAND LINE-CROSSING DETECTION
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 2.1: Understanding Line-Crossing Detection")
print("=" * 80)

"""
üìñ THEORY: Line-Crossing for People Counting

Concept:
- Define a virtual line in the video
- Track when objects cross the line
- Count direction (entering vs exiting)
- Maintain running totals

Why Line-Crossing?
‚úì Simple and effective
‚úì Works with any tracking system
‚úì Direction-aware (in vs out)
‚úì Handles multiple people simultaneously
‚úì Industry standard approach

==================================================

LINE-CROSSING ALGORITHM:

Components Needed:
1. Line definition: Two points (x1, y1) and (x2, y2)
2. Track positions: Current and previous frame
3. Crossing detection: Did trajectory cross line?
4. Direction detection: Which way did they cross?

Mathematical Approach:

Method 1: Line Segment Intersection
- Check if line segment (prev_pos, curr_pos) intersects counting line
- Use cross product to determine intersection
- Determine direction using relative positions

Method 2: Signed Distance
- Calculate perpendicular distance from point to line
- Sign indicates which side of line
- Crossing = sign change between frames
- Direction = positive to negative vs negative to positive

==================================================

GEOMETRIC FORMULA:

For line from point A(x1, y1) to B(x2, y2)
And object center at point P(x, y)

Cross Product Method:
- v1 = (x2 - x1, y2 - y1)  # Line vector
- v2 = (x - x1, y - y1)    # Point vector
- cross = v1[0] * v2[1] - v1[1] * v2[0]
- cross > 0: Point on one side
- cross < 0: Point on other side
- cross = 0: Point on line

Crossing Detection:
- If sign(cross_prev) ‚â† sign(cross_curr): CROSSED!
- Direction: cross_prev < 0 and cross_curr > 0 ‚Üí Direction 1
- Direction: cross_prev > 0 and cross_curr < 0 ‚Üí Direction 2

==================================================

LINE PLACEMENT STRATEGIES:

Entrance/Exit Monitoring:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ‚îÇ
‚îÇ     ‚Üì‚Üì‚Üì  ENTRANCE  ‚Üë‚Üë‚Üë ‚îÇ
‚îÇ     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ  ‚Üê Counting line
‚îÇ                        ‚îÇ
‚îÇ      MONITORED AREA    ‚îÇ
‚îÇ                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Placement Tips:
‚úì Place perpendicular to traffic flow
‚úì Avoid areas with frequent stops
‚úì Clear sightline (no obstructions)
‚úì Adequate lighting
‚úì Consider camera angle

Multiple Lines:
- Line 1: Main entrance
- Line 2: Secondary entrance
- Line 3: Exit-only door
- Total = sum of all lines

==================================================

COUNTING SCENARIOS:

Scenario 1: Single Entrance/Exit (Bidirectional)
- One line, count both directions
- IN: crosses line one way
- OUT: crosses line other way
- Net count = IN - OUT

Scenario 2: Separate Entrance/Exit
- Two lines (one per door)
- Line 1: Only count crossings in
- Line 2: Only count crossings out
- Total IN, Total OUT tracked separately

Scenario 3: Zone Counting
- Define polygon zone
- Count entries (any line crossing into zone)
- Count exits (any line crossing out of zone)
- Current occupancy = entries - exits

Scenario 4: Bidirectional Traffic
- Busy corridor, people going both ways
- Line in middle of corridor
- Count both directions
- Analyze traffic patterns

==================================================

HANDLING EDGE CASES:

Case 1: Person Lingers on Line
- Problem: Multiple crossings detected
- Solution: Cooldown period per track
- Only count once per track per X seconds

Case 2: Person Crosses Back and Forth
- Problem: Inflated count
- Solution: Track last crossing direction
- Only count if direction changes

Case 3: Multiple People Cross Simultaneously
- Problem: Accurate counting
- Solution: Track each person's ID separately
- Count each unique crossing

Case 4: Person Partially Crosses
- Problem: False positive
- Solution: Require full crossing
- Check both feet/bottom of bbox crossed

==================================================

PERFORMANCE CONSIDERATIONS:

Accuracy Factors:
- Tracking quality (ID consistency)
- Line placement (optimal location)
- Camera angle (perpendicular is best)
- Lighting conditions
- Occlusions at line

Expected Accuracy:
- Optimal conditions: 95-98%
- Normal conditions: 90-95%
- Challenging conditions: 85-90%

Common Errors:
- Missed counts: Track lost at line
- Double counts: ID switch at line
- Wrong direction: Tracking jitter near line
"""

print("""
üìê VISUAL REPRESENTATION:

Line-Crossing Detection:

Frame N-1:                Frame N:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ
‚îÇ    ‚óè        ‚îÇ          ‚îÇ             ‚îÇ
‚îÇ   P1        ‚îÇ          ‚îÇ      ‚óè      ‚îÇ
‚îÇ             ‚îÇ          ‚îÇ     P2      ‚îÇ
‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE     ‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚îÇ LINE
‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ
‚îÇ             ‚îÇ          ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

P1 above line (cross < 0)    P2 below line (cross > 0)
                    ‚Üí CROSSING DETECTED! ‚úì

Direction Determination:
- P1 to P2: Negative to Positive ‚Üí Direction: DOWN (or IN)
- P2 to P1: Positive to Negative ‚Üí Direction: UP (or OUT)

==================================================

IMPLEMENTATION STEPS:

1. Define line coordinates
   line_start = (x1, y1)
   line_end = (x2, y2)

2. Track object centers
   track_positions[track_id] = [(x, y), ...]

3. Calculate cross product
   cross_prev = calculate_cross_product(prev_pos, line)
   cross_curr = calculate_cross_product(curr_pos, line)

4. Detect crossing
   if sign(cross_prev) != sign(cross_curr):
       crossing_detected = True

5. Determine direction
   if cross_prev < 0 and cross_curr > 0:
       direction = "ENTERING"
   elif cross_prev > 0 and cross_curr < 0:
       direction = "EXITING"

6. Update counters
   if direction == "ENTERING":
       count_in += 1
   elif direction == "EXITING":
       count_out += 1
   
   current_occupancy = count_in - count_out
""")

print("\n‚úÖ Exercise 2.1 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 2.2: IMPLEMENT LINE-CROSSING DETECTION
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 2.2: Implement Line-Crossing Detection")
print("=" * 80)

"""
üìñ THEORY: Implementation Details

We'll implement:
1. Line definition (interactive or fixed)
2. Cross product calculation
3. Crossing detection logic
4. Direction determination
5. Count tracking
6. Visualization

Data Structures:
- line_coords: (x1, y1, x2, y2)
- track_history: {track_id: [(x, y), ...]}
- crossed_tracks: {track_id: last_direction}
- counters: {in: 0, out: 0, current: 0}
"""

class PeopleCounter:
    """
    People counting system using line-crossing detection
    """
    
    def __init__(self, line_start, line_end):
        """
        Initialize counter with line coordinates
        
        Args:
            line_start: (x, y) tuple for line start point
            line_end: (x, y) tuple for line end point
        """
        self.line_start = np.array(line_start)
        self.line_end = np.array(line_end)
        
        # Track positions history (last 2 positions per track)
        self.track_positions = defaultdict(lambda: deque(maxlen=2))
        
        # Tracks that have crossed (to prevent double counting)
        self.crossed_tracks = {}
        
        # Counters
        self.count_in = 0
        self.count_out = 0
        self.total_crossings = 0
        
        print(f"‚úÖ PeopleCounter initialized")
        print(f"   Line: {line_start} ‚Üí {line_end}")
    
    def _calculate_cross_product(self, point):
        """
        Calculate cross product to determine which side of line point is on
        
        Args:
            point: (x, y) tuple
            
        Returns:
            float: cross product (sign indicates side)
        """
        # Line vector
        line_vec = self.line_end - self.line_start
        
        # Point vector (from line start to point)
        point_vec = np.array(point) - self.line_start
        
        # Cross product (2D)
        cross = line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0]
        
        return cross
    
    def update(self, tracks):
        """
        Update counter with new tracks
        
        Args:
            tracks: List of tracks from DeepSORT
            
        Returns:
            dict: Crossing events {track_id: direction}
        """
        crossings = {}
        
        for track in tracks:
            if not track.is_confirmed():
                continue
            
            track_id = track.track_id
            bbox = track.to_ltrb()
            x1, y1, x2, y2 = bbox
            
            # Get center point (bottom-center for better accuracy)
            center_x = int((x1 + x2) / 2)
            center_y = int(y2)  # Bottom of bounding box
            center = (center_x, center_y)
            
            # Add to position history
            self.track_positions[track_id].append(center)
            
            # Need at least 2 positions to detect crossing
            if len(self.track_positions[track_id]) < 2:
                continue
            
            # Get previous and current positions
            prev_pos = self.track_positions[track_id][0]
            curr_pos = self.track_positions[track_id][1]
            
            # Calculate cross products
            cross_prev = self._calculate_cross_product(prev_pos)
            cross_curr = self._calculate_cross_product(curr_pos)
            
            # Detect crossing (sign change)
            if np.sign(cross_prev) != np.sign(cross_curr) and cross_prev != 0:
                
                # Check if already counted this track recently
                if track_id in self.crossed_tracks:
                    last_direction = self.crossed_tracks[track_id]
                    
                    # Determine current direction
                    if cross_prev < 0 and cross_curr > 0:
                        curr_direction = "IN"
                    else:
                        curr_direction = "OUT"
                    
                    # Only count if direction changed (prevents oscillation)
                    if curr_direction == last_direction:
                        continue
                
                # Determine direction
                if cross_prev < 0 and cross_curr > 0:
                    direction = "IN"
                    self.count_in += 1
                else:
                    direction = "OUT"
                    self.count_out += 1
                
                self.total_crossings += 1
                self.crossed_tracks[track_id] = direction
                crossings[track_id] = direction
        
        return crossings
    
    def get_counts(self):
        """Get current counts"""
        return {
            'in': self.count_in,
            'out': self.count_out,
            'current': self.count_in - self.count_out,
            'total': self.total_crossings
        }
    
    def draw_line(self, frame):
        """
        Draw counting line on frame
        
        Args:
            frame: Input frame
            
        Returns:
            Annotated frame
        """
        annotated = frame.copy()
        
        # Draw line
        cv2.line(annotated, tuple(self.line_start.astype(int)), 
                tuple(self.line_end.astype(int)), (0, 255, 255), 3)
        
        # Draw endpoints
        cv2.circle(annotated, tuple(self.line_start.astype(int)), 8, (0, 255, 255), -1)
        cv2.circle(annotated, tuple(self.line_end.astype(int)), 8, (0, 255, 255), -1)
        
        # Add label
        mid_point = ((self.line_start + self.line_end) / 2).astype(int)
        cv2.putText(annotated, "COUNTING LINE", tuple(mid_point - [0, 15]),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        return annotated
    
    def draw_counts(self, frame):
        """
        Draw count information on frame
        
        Args:
            frame: Input frame
            
        Returns:
            Annotated frame
        """
        annotated = frame.copy()
        counts = self.get_counts()
        
        # Semi-transparent panel
        overlay = annotated.copy()
        cv2.rectangle(overlay, (10, 200), (350, 400), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, annotated, 0.4, 0, annotated)
        
        # Draw counts
        y_offset = 240
        cv2.putText(annotated, "PEOPLE COUNT", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        y_offset += 40
        cv2.putText(annotated, f"IN:  {counts['in']}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        
        y_offset += 40
        cv2.putText(annotated, f"OUT: {counts['out']}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
        
        y_offset += 40
        cv2.putText(annotated, f"Current: {counts['current']}", (20, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)
        
        return annotated

print("‚úÖ Class created: PeopleCounter")
print("\nüìä Features:")
print("   ‚Ä¢ Line-crossing detection (cross product method)")
print("   ‚Ä¢ Direction-aware counting (IN vs OUT)")
print("   ‚Ä¢ Duplicate prevention (track last crossing)")
print("   ‚Ä¢ Oscillation handling (direction change required)")
print("   ‚Ä¢ Current occupancy calculation (IN - OUT)")
print("   ‚Ä¢ Visualization (line + counters)")

print("\nüß™ Testing PeopleCounter class...")

# Create test counter (horizontal line in middle of 720p frame)
test_counter = PeopleCounter(
    line_start=(100, 360),  # Left side, middle height
    line_end=(1180, 360)    # Right side, middle height
)

print(f"\n‚úÖ Test counter created!")
print(f"   Initial counts: {test_counter.get_counts()}")

print("\n‚úÖ Exercise 2.2 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 2.3: PEOPLE COUNTING DEMO
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 2.3: People Counting Demo with Webcam")
print("=" * 80)

"""
üìñ THEORY: Complete Counting System

Integration Steps:
1. Initialize YOLO detector
2. Initialize DeepSORT tracker
3. Initialize PeopleCounter
4. For each frame:
   a. Detect with YOLO
   b. Track with DeepSORT
   c. Update counter
   d. Visualize everything
5. Display results

What to Observe:
- People get unique IDs
- IDs persist as they move
- Counter increments when crossing line
- Direction is detected (IN vs OUT)
- Current occupancy updates
- Visualization clear and informative
"""

print("""
üé• COMPLETE PEOPLE COUNTING DEMO

Below is the full code for webcam people counting.
This integrates YOLO + DeepSORT + PeopleCounter!

Features:
‚úì Real-time detection + tracking
‚úì Line-crossing detection
‚úì Direction-aware counting (IN/OUT)
‚úì Current occupancy tracking
‚úì Visual counting line
‚úì Counter display panel
‚úì FPS monitoring

üìù To run this demo:
1. Copy the code below to a new cell
2. Execute the cell
3. Your webcam will open
4. Walk across the yellow line to test counting!
5. Try going both directions (IN and OUT)
6. Press 'q' to quit

‚ö†Ô∏è  Note: Adjust line position based on your camera view
         Default line is horizontal in middle of frame
""")

print("\n" + "=" * 80)
print("CODE: PEOPLE COUNTING DEMO")
print("=" * 80)

print("""
Copy this code to a new cell to run people counting:

-----------------------------------------------------------------------
import cv2
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
import time

# Load models
print("Loading models...")
model = YOLO('yolov8n.pt')
tracker = DeepSort(max_age=30, n_init=3, embedder="mobilenet", embedder_gpu=False)

# Initialize counter (adjust line position for your camera!)
# Line coordinates: (x1, y1) to (x2, y2)
# Default: horizontal line in middle of 640x480 frame
counter = PeopleCounter(
    line_start=(50, 240),   # Left side, middle height
    line_end=(590, 240)     # Right side, middle height
)

# Open webcam
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("‚ùå Cannot open webcam!")
else:
    print("‚úÖ Webcam opened!")
    print("üé• Starting people counting... (Press 'q' to quit)")
    print("üí° Walk across the YELLOW line to test counting!")
    
    # FPS tracking
    fps_counter = 0
    start_time = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. YOLO Detection
        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)
        detections = results[0].boxes
        
        # 2. Convert to DeepSORT format
        deepsort_input = []
        for box in detections:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            w = x2 - x1
            h = y2 - y1
            deepsort_input.append(([x1, y1, w, h], conf, 'person'))
        
        # 3. Update tracker
        tracks = tracker.update_tracks(deepsort_input, frame=frame)
        
        # 4. Update counter (detect crossings)
        crossings = counter.update(tracks)
        
        # 5. Visualize tracks
        for track in tracks:
            if not track.is_confirmed():
                continue
            
            track_id = track.track_id
            bbox = track.to_ltrb()
            x1, y1, x2, y2 = map(int, bbox)
            
            # Color: Green normally, Red if just crossed
            if track_id in crossings:
                color = (0, 0, 255)  # Red
                direction = crossings[track_id]
                label = f'ID: {track_id} ({direction})'
            else:
                color = (0, 255, 0)  # Green
                label = f'ID: {track_id}'
            
            # Draw bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # Draw ID
            cv2.putText(frame, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # 6. Draw counting line
        frame = counter.draw_line(frame)
        
        # 7. Draw counter panel
        frame = counter.draw_counts(frame)
        
        # 8. Calculate FPS
        fps_counter += 1
        if fps_counter % 30 == 0:
            elapsed = time.time() - start_time
            fps = 30 / elapsed
            start_time = time.time()
        
        # 9. Display FPS
        cv2.putText(frame, f'FPS: {fps:.1f}', (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        # 10. Show frame
        cv2.imshow('People Counting System', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    # Final statistics
    final_counts = counter.get_counts()
    print(f"\\nüìä Final Statistics:")
    print(f"   ‚Ä¢ People IN: {final_counts['in']}")
    print(f"   ‚Ä¢ People OUT: {final_counts['out']}")
    print(f"   ‚Ä¢ Current occupancy: {final_counts['current']}")
    print(f"   ‚Ä¢ Total crossings: {final_counts['total']}")
    print(f"   ‚Ä¢ Average FPS: {fps:.1f}")
-----------------------------------------------------------------------
""")

print("\nüí° What to Observe:")
print("   ‚Ä¢ Yellow horizontal line in middle of frame")
print("   ‚Ä¢ Each person gets unique green box + ID")
print("   ‚Ä¢ Box turns RED when crossing line")
print("   ‚Ä¢ Counter panel shows IN, OUT, Current")
print("   ‚Ä¢ IN increases when crossing one direction")
print("   ‚Ä¢ OUT increases when crossing other direction")
print("   ‚Ä¢ Current = IN - OUT (occupancy)")

print("\nüß™ Test Scenarios:")
print("   1. Walk across line left-to-right (should count IN)")
print("   2. Walk across line right-to-left (should count OUT)")
print("   3. Walk back and forth (alternates IN/OUT)")
print("   4. Multiple people cross together (each counted)")
print("   5. Person lingers on line (counted only once)")

print("\n‚úÖ Exercise 2.3 Complete!")
print("=" * 80)


print("\n" + "=" * 80)
print("üè¢ PART 3: ZONE-BASED MONITORING")
print("=" * 80)


# ==================================================
# EXERCISE 3.1: UNDERSTAND ZONE-BASED MONITORING
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 3.1: Understanding Zone-Based Monitoring")
print("=" * 80)

"""
üìñ THEORY: Zone-Based Tracking and Monitoring

What is Zone Monitoring?
- Define specific areas (zones) in camera view
- Track which people are in which zones
- Count occupancy per zone
- Detect unauthorized access
- Alert on capacity violations

Why Zone Monitoring?
‚úì Restricted area access control
‚úì Capacity management per area
‚úì Safety zone monitoring (hazard areas)
‚úì Heat map generation (where people spend time)
‚úì Traffic flow analysis

==================================================

ZONE DEFINITION METHODS:

Method 1: Rectangular Zones
- Simplest approach
- Define by (x1, y1, x2, y2) corners
- Check if point inside rectangle
- Fast computation
- Limited flexibility

Method 2: Polygonal Zones
- More flexible shapes
- Define by list of vertices [(x1,y1), (x2,y2), ...]
- Point-in-polygon algorithm
- Can match room layouts
- Slightly slower

Method 3: Multiple Zones
- Divide space into regions
- Each zone has properties (name, capacity, restricted)
- Track which people in which zones
- Enable complex monitoring rules

==================================================

POINT-IN-POLYGON ALGORITHM:

Ray Casting Method:
1. Draw ray from point to infinity (horizontal)
2. Count intersections with polygon edges
3. Odd count = inside polygon
4. Even count = outside polygon

OpenCV Implementation:
- cv2.pointPolygonTest(contour, point, measureDist)
- Returns: >0 inside, =0 on edge, <0 outside
- Fast and reliable
- Built-in to OpenCV

==================================================

ZONE MONITORING USE CASES:

Security System:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Public Area                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ Restricted‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ   Zone    ‚îÇ  ‚Üê Authorized only   ‚îÇ
‚îÇ  ‚îÇ  (Admin)  ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  Meeting Room   ‚îÇ ‚Üê Max 10 people‚îÇ
‚îÇ  ‚îÇ  (Capacity: 10) ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Factory Safety:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                     ‚îÇ
‚îÇ    ‚ö†Ô∏è  Hazard Zone ‚ö†Ô∏è               ‚îÇ
‚îÇ    (Require PPE)                    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ
‚îÇ    Safe Work Area                  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Retail Store:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇDept1‚îÇ  ‚îÇDept2‚îÇ  ‚îÇDept3‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         Checkout Area               ‚îÇ
‚îÇ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
- Track dwell time per department
- Queue length at checkout
- Traffic patterns

==================================================

ZONE PROPERTIES:

Basic Properties:
- name: "Meeting Room A"
- vertices: [(x1,y1), (x2,y2), ...]
- capacity: 10 (max people)
- restricted: True/False

Monitoring Properties:
- current_count: 3 (people currently in zone)
- total_entries: 47 (cumulative)
- total_exits: 44
- avg_dwell_time: 5.2 minutes

Alert Triggers:
- Over capacity: current_count > capacity
- Unauthorized: person_id not in authorized_list
- Too long: dwell_time > max_allowed
- Safety: no_ppe_detected in hazard_zone

==================================================

IMPLEMENTATION APPROACH:

Data Structures:

Zone Class:
class Zone:
    - name
    - vertices (polygon points)
    - capacity
    - restricted
    - current_occupants (set of track IDs)
    - entry_times (dict: track_id -> timestamp)
    - total_entries
    - total_exits

Methods:
- contains_point(point) ‚Üí bool
- update(tracks) ‚Üí events dict
- get_occupancy() ‚Üí int
- is_over_capacity() ‚Üí bool
- get_dwell_times() ‚Üí dict
- draw(frame) ‚Üí annotated frame

==================================================

ALGORITHM FLOW:

For each frame:
1. Get tracked objects from DeepSORT
2. For each track:
   a. Get object center point (x, y)
   b. For each zone:
      - Check if point inside zone
      - If inside:
        * Add to current_occupants
        * If new: record entry time, increment entries
      - If not inside but was before:
        * Remove from current_occupants
        * Delete entry time, increment exits
3. Check violations:
   - Occupancy > capacity?
   - Track in restricted zone?
   - Dwell time too long?
4. Generate alerts if violations detected
5. Visualize zones on frame

==================================================

VISUALIZATION TECHNIQUES:

1. Semi-transparent overlays
   ‚Ä¢ cv2.fillPoly() with transparency
   ‚Ä¢ Different colors per zone
   ‚Ä¢ Doesn't obscure video

2. Zone labels
   ‚Ä¢ Name at centroid
   ‚Ä¢ Current occupancy count
   ‚Ä¢ Capacity status (color-coded)

3. Track indicators
   ‚Ä¢ Highlight tracks inside zones
   ‚Ä¢ Show which zone each person is in
   ‚Ä¢ Entry/exit animations

4. Alert overlays
   ‚Ä¢ Flashing border for violations
   ‚Ä¢ Text alerts on screen
   ‚Ä¢ Color changes (red for violations)

==================================================

PERFORMANCE CONSIDERATIONS:

Speed:
- Point-in-polygon: O(n) where n = vertices
- Multiple zones: O(m √ó t) where m = zones, t = tracks
- Negligible overhead for <10 zones

Accuracy:
- Use bottom-center of bbox (person's feet)
- More accurate than bbox center
- Handles partially inside zones

Memory:
- Track history per zone
- Limit stored history (e.g., last 100 entries)
- Periodic cleanup of old data

==================================================

ADVANCED FEATURES:

1. Zone Hierarchies
   ‚Ä¢ Nested zones (room inside building)
   ‚Ä¢ Parent-child relationships
   ‚Ä¢ Inheritance of properties

2. Time-based Rules
   ‚Ä¢ Zone restrictions by time of day
   ‚Ä¢ Capacity limits vary by schedule
   ‚Ä¢ Automated alerts during specific hours

3. Heat Maps
   ‚Ä¢ Accumulate occupancy over time
   ‚Ä¢ Visualize popular areas
   ‚Ä¢ Optimize layout based on traffic

4. Path Analysis
   ‚Ä¢ Common entry/exit paths
   ‚Ä¢ Time to traverse zones
   ‚Ä¢ Bottleneck identification

5. Multi-camera Zones
   ‚Ä¢ Zones span multiple cameras
   ‚Ä¢ Track handoff between cameras
   ‚Ä¢ Global zone monitoring
"""

print("""
üìä ZONE MONITORING EXAMPLE:

Scenario: Office Building Floor

Zones Defined:
1. Lobby (public, unlimited capacity)
2. Meeting Room A (capacity: 10)
3. Meeting Room B (capacity: 6)
4. Server Room (restricted, capacity: 2)
5. Executive Area (restricted, capacity: 5)

Tracking in Action:
- Person ID 1 enters Lobby ‚Üí No alert
- Person ID 1 enters Meeting Room A ‚Üí Entry logged
- 11 people in Meeting Room A ‚Üí CAPACITY ALERT
- Person ID 5 enters Server Room ‚Üí RESTRICTED ALERT (if unauthorized)
- Person ID 3 in Server Room for 45 min ‚Üí DWELL TIME ALERT

System Response:
- Real-time alerts displayed
- Email sent to security
- Log entry created
- Dashboard updated

Benefits:
‚úì Automated capacity management
‚úì Security breach detection
‚úì Occupancy analytics
‚úì Safety compliance
‚úì Resource optimization
""")

print("\n‚úÖ Exercise 3.1 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 3.2: IMPLEMENT ZONE MONITORING
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 3.2: Implement Zone Monitoring System")
print("=" * 80)

"""
üìñ THEORY: Zone Monitoring Implementation

Data Structures Needed:
1. Zone definition (polygon vertices)
2. Zone properties (name, capacity, restricted)
3. Track-to-zone mapping
4. Occupancy counters per zone
5. Alert triggers

Implementation Steps:
1. Define zone polygon
2. Check if point is inside polygon (cv2.pointPolygonTest)
3. Track which objects in which zones
4. Count occupancy
5. Trigger alerts on violations
"""

class Zone:
    """
    Represents a monitoring zone (polygon area)
    """
    
    def __init__(self, name, vertices, capacity=None, restricted=False, color=(255, 0, 0)):
        """
        Initialize zone
        
        Args:
            name: Zone name (e.g., "Meeting Room A")
            vertices: List of (x, y) points defining polygon
            capacity: Max occupancy (None = unlimited)
            restricted: Whether zone requires authorization
            color: BGR color for visualization
        """
        self.name = name
        self.vertices = np.array(vertices, dtype=np.int32)
        self.capacity = capacity
        self.restricted = restricted
        self.color = color
        
        # Tracking
        self.current_occupants = set()  # Track IDs currently in zone
        self.entry_times = {}  # Track ID -> entry timestamp
        self.total_entries = 0
        self.total_exits = 0
        
        print(f"‚úÖ Zone created: {name}")
        print(f"   Vertices: {len(vertices)} points")
        print(f"   Capacity: {capacity if capacity else 'Unlimited'}")
        print(f"   Restricted: {restricted}")
    
    def contains_point(self, point):
        """
        Check if point is inside zone polygon
        
        Args:
            point: (x, y) tuple
            
        Returns:
            bool: True if inside, False if outside
        """
        result = cv2.pointPolygonTest(self.vertices, point, False)
        return result >= 0  # >= 0 means inside or on edge
    
    def update(self, tracks):
        """
        Update zone occupancy based on current tracks
        
        Args:
            tracks: List of tracks from DeepSORT
            
        Returns:
            dict: Events {track_id: event_type}
        """
        events = {}
        current_frame_occupants = set()
        
        for track in tracks:
            if not track.is_confirmed():
                continue
            
            track_id = track.track_id
            bbox = track.to_ltrb()
            x1, y1, x2, y2 = bbox
            
            # Use bottom-center of bbox (person's feet position)
            center_x = int((x1 + x2) / 2)
            center_y = int(y2)
            point = (center_x, center_y)
            
            # Check if inside zone
            if self.contains_point(point):
                current_frame_occupants.add(track_id)
                
                # New entry?
                if track_id not in self.current_occupants:
                    self.current_occupants.add(track_id)
                    self.entry_times[track_id] = time.time()
                    self.total_entries += 1
                    events[track_id] = "ENTERED"
        
        # Check for exits
        exited = self.current_occupants - current_frame_occupants
        for track_id in exited:
            self.current_occupants.discard(track_id)
            if track_id in self.entry_times:
                del self.entry_times[track_id]
            self.total_exits += 1
            events[track_id] = "EXITED"
        
        return events
    
    def get_occupancy(self):
        """Get current occupancy count"""
        return len(self.current_occupants)
    
    def is_over_capacity(self):
        """Check if zone is over capacity"""
        if self.capacity is None:
            return False
        return self.get_occupancy() > self.capacity
    
    def get_dwell_times(self):
        """Get dwell times for current occupants"""
        current_time = time.time()
        return {
            track_id: current_time - entry_time
            for track_id, entry_time in self.entry_times.items()
        }
    
    def draw(self, frame, show_info=True):
        """
        Draw zone on frame
        
        Args:
            frame: Input frame
            show_info: Whether to show zone info text
            
        Returns:
            Annotated frame
        """
        annotated = frame.copy()
        
        # Draw polygon with transparency
        overlay = annotated.copy()
        cv2.fillPoly(overlay, [self.vertices], self.color)
        cv2.addWeighted(overlay, 0.3, annotated, 0.7, 0, annotated)
        
        # Draw border
        cv2.polylines(annotated, [self.vertices], True, self.color, 2)
        
        if show_info:
            # Calculate text position (centroid of polygon)
            M = cv2.moments(self.vertices)
            if M["m00"] != 0:
                cx = int(M["m10"] / M["m00"])
                cy = int(M["m01"] / M["m00"])
            else:
                cx, cy = self.vertices[0]
            
            # Zone name
            cv2.putText(annotated, self.name, (cx - 50, cy - 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # Occupancy
            occupancy = self.get_occupancy()
            capacity_text = f"/{self.capacity}" if self.capacity else ""
            occupancy_text = f"Occupancy: {occupancy}{capacity_text}"
            
            # Color based on capacity
            if self.is_over_capacity():
                text_color = (0, 0, 255)  # Red if over
            else:
                text_color = (0, 255, 0)  # Green if OK
            
            cv2.putText(annotated, occupancy_text, (cx - 50, cy),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
            
            # Restricted label
            if self.restricted:
                cv2.putText(annotated, "RESTRICTED", (cx - 50, cy + 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
        
        return annotated

print("\n‚úÖ Class created: Zone")
print("\nüìä Features:")
print("   ‚Ä¢ Polygon-based zone definition")
print("   ‚Ä¢ Point-in-polygon detection (cv2)")
print("   ‚Ä¢ Occupancy tracking with entry/exit events")
print("   ‚Ä¢ Capacity monitoring and alerts")
print("   ‚Ä¢ Dwell time calculation")
print("   ‚Ä¢ Visual overlay with transparency")

print("\n‚úÖ Exercise 3.2 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 3.3: ZONE MONITORING DEMO
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 3.3: Zone Monitoring Demo Code")
print("=" * 80)

"""
üìñ THEORY: Complete Zone Monitoring System

Integration:
1. Define zones (polygons)
2. Track objects with DeepSORT
3. Check which zone each object is in
4. Update zone occupancy
5. Detect violations (over capacity, restricted access)
6. Visualize zones and alerts

Real-world Use Cases:
- Meeting room capacity monitoring
- Restricted area access control
- Safety zone monitoring (hazard areas)
- Queue management
- Traffic flow analysis
"""

print("""
üè¢ COMPLETE ZONE MONITORING DEMO

Below is the full code for zone-based monitoring.
This combines tracking + zones + alerts!

Features:
‚úì Multiple zones (meeting room, restricted area, etc.)
‚úì Real-time occupancy per zone
‚úì Capacity alerts (over limit)
‚úì Restricted area alerts (unauthorized access)
‚úì Visual zone overlays
‚úì Entry/exit events

üìù To run this demo:
1. Copy the code below to a new cell
2. Adjust zone coordinates for your camera view
3. Execute the cell
4. Walk in/out of zones to test!
5. Press 'q' to quit

‚ö†Ô∏è  Note: Zone coordinates are for 640x480 resolution
         Adjust vertices based on your camera!
""")

print("\n" + "=" * 80)
print("CODE: ZONE MONITORING DEMO")
print("=" * 80)

print("""
Copy this code to a new cell to run zone monitoring:

-----------------------------------------------------------------------
import cv2
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
import time

# Load models
print("Loading models...")
model = YOLO('yolov8n.pt')
tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0, embedder=None)

# Get camera dimensions
cap_temp = cv2.VideoCapture(0)
ret, frame_temp = cap_temp.read()
height, width = frame_temp.shape[:2]
cap_temp.release()

print(f"Camera: {width}x{height}")

# Define zones (adjust these coordinates for your camera!)
zones = [
    Zone(
        name="Meeting Room",
        vertices=[
            (100, 100),   # Top-left
            (300, 100),   # Top-right
            (300, 300),   # Bottom-right
            (100, 300)    # Bottom-left
        ],
        capacity=2,
        restricted=False,
        color=(0, 255, 0)  # Green
    ),
    Zone(
        name="Restricted Area",
        vertices=[
            (width-300, 100),
            (width-100, 100),
            (width-100, 300),
            (width-300, 300)
        ],
        capacity=1,
        restricted=True,
        color=(0, 0, 255)  # Red
    ),
]

print(f"\\nZones created: {len(zones)}")

# Open webcam
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("‚ùå Cannot open webcam!")
else:
    print("‚úÖ Webcam opened!")
    print("üé• Starting zone monitoring... (Press 'q' to quit)")
    
    fps_counter = 0
    start_time = time.time()
    fps = 0
    
    # Alert history
    alerts = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. YOLO Detection
        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)
        detections = results[0].boxes
        
        # 2. Convert to DeepSORT
        deepsort_input = []
        for box in detections:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            w, h = x2 - x1, y2 - y1
            deepsort_input.append(([x1, y1, w, h], conf, 'person'))
        
        # 3. Update tracker
        if len(deepsort_input) > 0:
            dummy_embeddings = [np.random.rand(128).astype(np.float32) for _ in deepsort_input]
            tracks = tracker.update_tracks(deepsort_input, embeds=dummy_embeddings, frame=frame)
        else:
            tracks = []
        
        # 4. Update zones
        for zone in zones:
            events = zone.update(tracks)
            
            # Check for alerts
            for track_id, event in events.items():
                if event == "ENTERED":
                    alert_msg = f"ID {track_id} entered {zone.name}"
                    alerts.append((time.time(), alert_msg))
                    print(f"üö™ {alert_msg}")
            
            # Capacity alert
            if zone.is_over_capacity():
                alert_msg = f"{zone.name} OVER CAPACITY ({zone.get_occupancy()}/{zone.capacity})"
                if not any(alert_msg in a[1] for a in alerts[-5:]):  # Avoid spam
                    alerts.append((time.time(), alert_msg))
                    print(f"‚ö†Ô∏è  {alert_msg}")
        
        # 5. Draw zones
        for zone in zones:
            frame = zone.draw(frame)
        
        # 6. Visualize tracks
        for track in tracks:
            if not track.is_confirmed():
                continue
            
            track_id = track.track_id
            bbox = track.to_ltrb()
            x1, y1, x2, y2 = map(int, bbox)
            
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # 7. Display recent alerts
        y_offset = 30
        for i, (alert_time, alert_msg) in enumerate(alerts[-5:]):  # Last 5 alerts
            age = time.time() - alert_time
            if age < 5:  # Show for 5 seconds
                alpha = 1 - (age / 5)  # Fade out
                cv2.putText(frame, f"‚ö†Ô∏è {alert_msg}", (10, y_offset),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                y_offset += 25
        
        # 8. Calculate FPS
        fps_counter += 1
        if fps_counter % 30 == 0:
            elapsed = time.time() - start_time
            fps = 30 / elapsed
            start_time = time.time()
        
        # 9. Display FPS
        cv2.putText(frame, f'FPS: {fps:.1f}', (width - 150, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        # 10. Show frame
        cv2.imshow('Zone Monitoring System', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    # Final statistics
    print(f"\\nüìä Final Zone Statistics:")
    for zone in zones:
        print(f"\\n{zone.name}:")
        print(f"   ‚Ä¢ Total entries: {zone.total_entries}")
        print(f"   ‚Ä¢ Total exits: {zone.total_exits}")
        print(f"   ‚Ä¢ Current occupancy: {zone.get_occupancy()}")
-----------------------------------------------------------------------
""")

print("\nüí° What to Observe:")
print("   ‚Ä¢ Colored zone overlays (green, red)")
print("   ‚Ä¢ Zone names and occupancy displayed")
print("   ‚Ä¢ Alerts appear when entering zones")
print("   ‚Ä¢ Capacity warnings if zone over limit")
print("   ‚Ä¢ Entry/exit events logged")

print("\nüß™ Test Scenarios:")
print("   1. Walk into green zone (meeting room)")
print("   2. Stay in zone (see occupancy count)")
print("   3. Walk into red zone (restricted area)")
print("   4. Exceed capacity (enter with 2+ people)")
print("   5. Walk out (exit event, occupancy decreases)")

print("\n‚úÖ Exercise 3.3 Complete!")
print("=" * 80)


print("\n" + "=" * 80)
print("üéØ PART 4: KEY TAKEAWAYS & NEXT STEPS")
print("=" * 80)


# ==================================================
# EXERCISE 4.1: DAY 24 SUMMARY
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 4.1: Day 24 Summary")
print("=" * 80)

print("""
üìö WHAT WE LEARNED TODAY:

‚úÖ DeepSORT Parameter Tuning:
   ‚Ä¢ Understood max_age (occlusion tolerance)
   ‚Ä¢ Understood n_init (confirmation threshold)
   ‚Ä¢ Understood max_iou_distance (matching strictness)
   ‚Ä¢ Created testing framework for parameter comparison
   ‚Ä¢ Learned how to tune for different scenarios

‚úÖ Occlusion Handling:
   ‚Ä¢ Learned how Kalman filter predicts during occlusion
   ‚Ä¢ Understood track state management (tentative, confirmed, lost)
   ‚Ä¢ Learned appearance matching for re-identification
   ‚Ä¢ Simulated occlusion scenarios
   ‚Ä¢ Determined optimal max_age values

‚úÖ Line-Crossing Detection:
   ‚Ä¢ Implemented cross-product method for line crossing
   ‚Ä¢ Created PeopleCounter class with direction detection
   ‚Ä¢ Handled edge cases (oscillation, lingering)
   ‚Ä¢ Built bidirectional counting (IN vs OUT)
   ‚Ä¢ Calculated current occupancy (IN - OUT)

‚úÖ Zone-Based Monitoring:
   ‚Ä¢ Implemented polygon zone definition
   ‚Ä¢ Used point-in-polygon detection (cv2)
   ‚Ä¢ Created Zone class with occupancy tracking
   ‚Ä¢ Implemented entry/exit events
   ‚Ä¢ Built capacity monitoring and alerts
   ‚Ä¢ Calculated dwell times per zone

‚úÖ Webcam Testing:
   ‚Ä¢ Tested tracking on MacBook webcam
   ‚Ä¢ Verified line-crossing detection works
   ‚Ä¢ Tracked multiple object types (people, cats)
   ‚Ä¢ Color-coded visualization working
   ‚Ä¢ Achieved real-time performance

üìä KEY METRICS TODAY:
   ‚Ä¢ Parameter configurations tested: 5
   ‚Ä¢ Occlusion scenarios analyzed: 4
   ‚Ä¢ Line-crossing accuracy: ~95%
   ‚Ä¢ Zone monitoring classes created: 2
   ‚Ä¢ Webcam test: Successful ‚úì
   ‚Ä¢ FPS achieved: 20-30 (CPU)

üí° KEY INSIGHTS:

   1. Parameter tuning is scenario-dependent
      ‚Üí Office: max_age=20-30, n_init=2-3
      ‚Üí Factory: max_age=40-60, n_init=4-5
      ‚Üí Adjust based on real deployment conditions
      
   2. Line-crossing is simple but powerful
      ‚Üí Cross-product method is efficient
      ‚Üí Direction detection enables traffic analysis
      ‚Üí Handles multiple people simultaneously
      
   3. Zone monitoring adds spatial context
      ‚Üí Polygon zones more flexible than rectangles
      ‚Üí Occupancy tracking enables capacity management
      ‚Üí Dwell time useful for behavior analysis
      
   4. Tracking works without embedder
      ‚Üí Dummy embeddings sufficient for basic tracking
      ‚Üí Appearance features improve re-ID but not critical
      ‚Üí Motion-based tracking handles most scenarios
      
   5. Real-time performance achievable
      ‚Üí 20-30 FPS on CPU is usable
      ‚Üí Visualization overhead is minimal
      ‚Üí GPU would give 40-60 FPS
""")

print("=" * 80)

print("\n‚úÖ Exercise 4.1 Complete!")
print("=" * 80)


# ==================================================
# EXERCISE 4.2: TOMORROW'S PLAN (DAY 25)
# ==================================================

print("\n" + "=" * 80)
print("EXERCISE 4.2: Tomorrow's Plan")
print("=" * 80)

print("""
üéØ DAY 25: VIDEO PROCESSING PIPELINE (November 20, 2025)

What we'll do:
1. Build production video processing pipeline
   ‚Ä¢ Multi-threaded video reading
   ‚Ä¢ Frame queue management
   ‚Ä¢ FPS control and synchronization
   ‚Ä¢ Memory management

2. Handle multiple video sources
   ‚Ä¢ Video files (MP4, AVI, MOV)
   ‚Ä¢ Webcam input
   ‚Ä¢ RTSP streams (IP cameras)
   ‚Ä¢ Multiple cameras simultaneously

3. Implement frame skipping strategies
   ‚Ä¢ Skip every Nth frame if needed
   ‚Ä¢ Adaptive frame rate based on detection load
   ‚Ä¢ Balance accuracy vs speed

4. Output video processing
   ‚Ä¢ Save annotated videos
   ‚Ä¢ Export detection logs (CSV, JSON)
   ‚Ä¢ Generate summary reports
   ‚Ä¢ Create highlight clips

5. Batch processing capabilities
   ‚Ä¢ Process multiple videos
   ‚Ä¢ Parallel processing
   ‚Ä¢ Progress tracking
   ‚Ä¢ Error handling

6. Performance optimization
   ‚Ä¢ GPU acceleration (if available)
   ‚Ä¢ Batch inference
   ‚Ä¢ Resolution optimization
   ‚Ä¢ Memory leak prevention

Expected outcomes:
   ‚Ä¢ Robust video processing pipeline
   ‚Ä¢ Handle various input formats
   ‚Ä¢ Process videos efficiently
   ‚Ä¢ Export results in multiple formats
   ‚Ä¢ 30 FPS sustained performance
   ‚Ä¢ Production-ready code structure

Tech Stack:
   ‚Ä¢ OpenCV (video I/O)
   ‚Ä¢ Threading/multiprocessing (parallel processing)
   ‚Ä¢ Queue (frame buffering)
   ‚Ä¢ JSON/CSV (export formats)
   ‚Ä¢ tqdm (progress bars)

Time estimate: 5-6 hours
""")

print("=" * 80)

print("\n‚úÖ Exercise 4.2 Complete!")
print("=" * 80)


print("\n" + "=" * 80)
print("DAY 24 COMPLETE! ‚úÖ")
print("=" * 80)

print("""
OBJECTIVES ACHIEVED:
   ‚úÖ Tuned DeepSORT parameters for different scenarios
   ‚úÖ Handled occlusions robustly (max_age optimization)
   ‚úÖ Implemented line-crossing detection system
   ‚úÖ Created PeopleCounter class with bidirectional counting
   ‚úÖ Built zone-based monitoring system
   ‚úÖ Implemented Zone class with capacity management
   ‚úÖ Tested on webcam (MacBook - successful!)
   ‚úÖ Tracked multiple object types (people, cats, etc.)
   ‚úÖ Achieved real-time performance (20-30 FPS)

üìä KEY METRICS:
   - Parameter configurations: 5 tested
   - Counting accuracy: ~95%
   - Zone detection: Point-in-polygon working
   - Webcam FPS: 20-30 (CPU)
   - Object types tracked: All 80 COCO classes
   - Color visualization: 4 different colors

üí° KEY LEARNINGS:
   - Parameter tuning critical for different scenarios
   - Line-crossing simple but effective for counting
   - Zone monitoring adds powerful spatial awareness
   - Tracking works well without complex embeddings
   - Real-time performance achievable on CPU
   - Webcam testing validates implementation
   - Cross-product method elegant for line detection

üéØ TOMORROW (DAY 25):
   - Build production video processing pipeline
   - Handle multiple input formats
   - Implement batch processing
   - Export results (CSV, JSON, video)
   - Optimize for sustained 30 FPS
   - Create modular, reusable code

üíæ FILES CREATED TODAY:
   - day24_tracking_optimization.ipynb (Complete!)
   - Classes: TrackerConfig, PeopleCounter, Zone
   - Functions: yolo_to_deepsort, simulate_occlusion_scenario
   - Webcam demos: People counting, zone monitoring
   - Theory: Parameters, occlusion, line-crossing, zones

üî• PROGRESS UPDATE:
   Week 4: 43% complete (3/7 days)
   Overall: 14.3% complete (24/168 days)
   
üöÄ MOMENTUM:
   ‚úÖ Week 1: Neural Networks (Complete)
   ‚úÖ Week 2: YOLO Detection (Complete - 75.1% mAP)
   ‚úÖ Week 3: Medical Classifier (Complete - 94.48%)
   ‚úÖ Day 22: Security System Planning (Complete)
   ‚úÖ Day 23: DeepSORT Integration (Complete)
   ‚úÖ Day 24: Tracking Optimization (Complete - TODAY!)
   
   Next: Video processing pipeline with multiple sources! üé¨
""")

print("=" * 80)



