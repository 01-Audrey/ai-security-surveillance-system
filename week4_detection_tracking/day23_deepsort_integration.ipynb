{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1cb3d-a0c9-43c0-80b5-fd85aabe671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "ML LEARNING JOURNEY - DAY 23\n",
    "==================================================\n",
    "Week: 4 of 24\n",
    "Day: 23 of 168\n",
    "Date: November 18, 2025\n",
    "Topic: Multi-Object Tracking with DeepSORT\n",
    "Overall Progress: 13.7%\n",
    "\n",
    "Week 4: Detection & Tracking Foundation\n",
    "âœ… Day 22: Project Planning & Architecture (COMPLETED)\n",
    "ğŸ”„ Day 23: Multi-Object Tracking (DeepSORT) (TODAY!)\n",
    "â¬œ Day 24: Tracking Optimization\n",
    "â¬œ Day 25: Video Processing Pipeline\n",
    "â¬œ Day 26: Testing & Performance\n",
    "â¬œ Day 27: Code Cleanup & Modularization\n",
    "â¬œ Day 28: Week 4 Review\n",
    "\n",
    "Progress: 29% (2/7 days)\n",
    "\n",
    "==================================================\n",
    "ğŸ¯ Week 4 Project: Security System - Detection & Tracking\n",
    "- Build real-time person detection system\n",
    "- Implement multi-object tracking with unique IDs\n",
    "- Achieve 30 FPS performance target\n",
    "- Foundation for face recognition (Week 5)\n",
    "\n",
    "ğŸ¯ Today's Learning Objectives:\n",
    "1. Understand DeepSORT tracking algorithm (Kalman filter, appearance descriptor, Hungarian matching)\n",
    "2. Install and configure DeepSORT implementation\n",
    "3. Integrate YOLO detection with DeepSORT tracking\n",
    "4. Track people across video frames with persistent IDs\n",
    "5. Handle occlusions and re-identification\n",
    "6. Visualize tracking results with bounding boxes and IDs\n",
    "7. Measure tracking performance and FPS impact\n",
    "\n",
    "ğŸ“š Today's Structure:\n",
    "   Part 1 (2h): DeepSORT Theory & Installation\n",
    "   Part 2 (2h): YOLO + DeepSORT Integration\n",
    "   Part 3 (2h): Visualization & Testing\n",
    "   Part 4 (1h): Summary & Next Steps\n",
    "\n",
    "ğŸ¯ SUCCESS CRITERIA:\n",
    "   âœ… DeepSORT algorithm understood (theory)\n",
    "   âœ… DeepSORT installed and configured\n",
    "   âœ… YOLO + DeepSORT integration working\n",
    "   âœ… Objects tracked with unique, persistent IDs\n",
    "   âœ… IDs maintained across frames and occlusions\n",
    "   âœ… Tracking visualization implemented\n",
    "   âœ… Performance benchmarked (FPS measured)\n",
    "   âœ… Ready for tracking optimization (Day 24)\n",
    "==================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea11a9ba-993e-4930-812d-6ed9695e48c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing DeepSORT and dependencies...\n",
      "â±ï¸  This may take 2-3 minutes...\n",
      "\n",
      "Installing deep-sort-realtime...\n",
      "Installing ultralytics...\n",
      "Installing filterpy...\n",
      "Installing scikit-learn...\n",
      "Installing scipy...\n",
      "Installing opencv-python...\n",
      "\n",
      "âœ… All libraries installed!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“š IMPORTING LIBRARIES\n",
      "================================================================================\n",
      "\n",
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ“Š Library versions:\n",
      "   â€¢ OpenCV: 4.12.0\n",
      "   â€¢ NumPy: 2.2.6\n",
      "   â€¢ Pandas: 2.3.2\n",
      "   â€¢ Ultralytics: Installed âœ“\n",
      "   â€¢ DeepSORT: Installed âœ“\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# INSTALL REQUIRED LIBRARIES\n",
    "# ==================================================\n",
    "\n",
    "print(\"ğŸ“¦ Installing DeepSORT and dependencies...\")\n",
    "print(\"â±ï¸  This may take 2-3 minutes...\\n\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Get the correct Python executable path (handles spaces)\n",
    "python_exe = sys.executable\n",
    "\n",
    "packages = [\n",
    "    'deep-sort-realtime',\n",
    "    'ultralytics',\n",
    "    'filterpy',\n",
    "    'scikit-learn',\n",
    "    'scipy',\n",
    "    'opencv-python'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([python_exe, '-m', 'pip', 'install', package, '-q'])\n",
    "\n",
    "print(\"\\nâœ… All libraries installed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ==================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“š IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Computer vision\n",
    "import cv2\n",
    "\n",
    "# Deep learning\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Tracking\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "print(\"\\nâœ… All libraries imported successfully!\")\n",
    "print(\"\\nğŸ“Š Library versions:\")\n",
    "print(f\"   â€¢ OpenCV: {cv2.__version__}\")\n",
    "print(f\"   â€¢ NumPy: {np.__version__}\")\n",
    "print(f\"   â€¢ Pandas: {pd.__version__}\")\n",
    "print(\"   â€¢ Ultralytics: Installed âœ“\")\n",
    "print(\"   â€¢ DeepSORT: Installed âœ“\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9013ca-7255-41ed-81e4-ab10003652c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“š PART 1: DEEPSORT THEORY & INSTALLATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“š PART 1: DEEPSORT THEORY & INSTALLATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf80c9d8-a761-4d3a-b602-398ad6a59f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.1: Understanding DeepSORT Algorithm\n",
      "================================================================================\n",
      "\n",
      "ğŸ” DEEPSORT ALGORITHM BREAKDOWN:\n",
      "\n",
      "DeepSORT = Detection-based Tracking with Deep Learning\n",
      "\n",
      "Original SORT (Simple Online Realtime Tracking):\n",
      "â””â”€â”€ Uses only motion information (bounding box positions)\n",
      "â””â”€â”€ Fast but loses track during occlusions\n",
      "\n",
      "DeepSORT = SORT + Deep Appearance Features:\n",
      "â””â”€â”€ Adds \"how the object looks\" (appearance)\n",
      "â””â”€â”€ Uses ReID (Re-Identification) network\n",
      "â””â”€â”€ Better handles occlusions and crowded scenes\n",
      "â””â”€â”€ Reduces ID switches\n",
      "\n",
      "==================================================\n",
      "\n",
      "DEEPSORT PIPELINE (5 Steps):\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1. INPUT: Detections from YOLO                  â”‚\n",
      "â”‚    [x, y, w, h, confidence, class]              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                   â”‚\n",
      "                   â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 2. KALMAN FILTER (Motion Prediction)            â”‚\n",
      "â”‚    â€¢ Predicts where object will be next frame   â”‚\n",
      "â”‚    â€¢ Based on position + velocity               â”‚\n",
      "â”‚    â€¢ Handles smooth, continuous motion          â”‚\n",
      "â”‚                                                  â”‚\n",
      "â”‚    State: [x, y, aspect_ratio, height,          â”‚\n",
      "â”‚            vx, vy, va, vh]                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                   â”‚\n",
      "                   â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 3. APPEARANCE DESCRIPTOR (Deep Learning)        â”‚\n",
      "â”‚    â€¢ Extract 128D feature vector from crop      â”‚\n",
      "â”‚    â€¢ ReID network (trained on person ReID)      â”‚\n",
      "â”‚    â€¢ Captures \"what object looks like\"          â”‚\n",
      "â”‚    â€¢ Similar people â†’ similar embeddings        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                   â”‚\n",
      "                   â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 4. MATCHING (Hungarian Algorithm)               â”‚\n",
      "â”‚    â€¢ Match predictions to new detections        â”‚\n",
      "â”‚    â€¢ Cost = Motion + Appearance                 â”‚\n",
      "â”‚    â€¢ Motion: IOU distance (overlap)             â”‚\n",
      "â”‚    â€¢ Appearance: Cosine distance (embeddings)   â”‚\n",
      "â”‚    â€¢ Assigns same ID to matched objects         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                   â”‚\n",
      "                   â†“\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 5. TRACK MANAGEMENT                             â”‚\n",
      "â”‚    â€¢ Create new track for unmatched detection   â”‚\n",
      "â”‚    â€¢ Update existing tracks with new detection  â”‚\n",
      "â”‚    â€¢ Delete tracks missing for max_age frames   â”‚\n",
      "â”‚    â€¢ Confirm tracks after min_hits detections   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "==================================================\n",
      "\n",
      "KEY COMPONENTS EXPLAINED:\n",
      "\n",
      "1. KALMAN FILTER (Motion Model)\n",
      "   Purpose: Predict object location in next frame\n",
      "\n",
      "   State Variables:\n",
      "   â€¢ Position: (x, y) - center of bounding box\n",
      "   â€¢ Size: (aspect_ratio, height)\n",
      "   â€¢ Velocity: (vx, vy, va, vh) - how fast moving\n",
      "\n",
      "   How it works:\n",
      "   â€¢ Predict: Use velocity to estimate next position\n",
      "   â€¢ Update: Correct prediction with actual detection\n",
      "   â€¢ Handles: Smooth, linear motion\n",
      "\n",
      "2. REID NETWORK (Appearance Model)\n",
      "   Purpose: Recognize same object even after occlusion\n",
      "\n",
      "   Architecture: CNN (MobileNet or ResNet)\n",
      "   Input: Cropped person image (128x256)\n",
      "   Output: 128D embedding vector\n",
      "\n",
      "   Training: Pre-trained on person re-identification\n",
      "   â€¢ Learns to extract discriminative features\n",
      "   â€¢ Same person â†’ similar embeddings\n",
      "   â€¢ Different people â†’ different embeddings\n",
      "\n",
      "3. COST MATRIX (Matching)\n",
      "   Purpose: Decide which detection matches which track\n",
      "\n",
      "   For each (track, detection) pair:\n",
      "   â€¢ Motion cost = 1 - IOU(predicted_box, detected_box)\n",
      "   â€¢ Appearance cost = 1 - cosine_similarity(embeddings)\n",
      "   â€¢ Total cost = Î»â‚ * motion + Î»â‚‚ * appearance\n",
      "\n",
      "   Lower cost = better match\n",
      "\n",
      "4. HUNGARIAN ALGORITHM (Assignment)\n",
      "   Purpose: Find optimal track-detection assignments\n",
      "\n",
      "   Problem: Given cost matrix, assign detections to tracks\n",
      "   Goal: Minimize total cost\n",
      "   Complexity: O(nÂ³)\n",
      "   Result: Each detection assigned to at most one track\n",
      "\n",
      "==================================================\n",
      "\n",
      "TRACK LIFECYCLE:\n",
      "\n",
      "NEW DETECTION (no matching track)\n",
      "    â†“\n",
      "TENTATIVE TRACK (unconfirmed, ID assigned but not shown)\n",
      "    â†“ (appears for min_hits consecutive frames)\n",
      "CONFIRMED TRACK (ID now visible, actively tracked)\n",
      "    â†“ (continues being detected)\n",
      "ACTIVE TRACKING (ID maintained, position updated)\n",
      "    â†“ (object becomes occluded/leaves frame)\n",
      "LOST TRACK (no detection, but kept for max_age frames)\n",
      "    â†“ (reappears: continue | doesn't reappear: delete)\n",
      "DELETED TRACK (removed from system)\n",
      "\n",
      "Key Parameters:\n",
      "- min_hits = 3: Need 3 consecutive detections to confirm\n",
      "- max_age = 30: Keep lost track for 30 frames (1 second @ 30fps)\n",
      "- max_iou_distance = 0.7: Matching threshold\n",
      "\n",
      "==================================================\n",
      "\n",
      "WHY DEEPSORT IS POWERFUL:\n",
      "\n",
      "âœ“ HANDLES OCCLUSIONS\n",
      "  â€¢ Track continues even when person hidden\n",
      "  â€¢ Uses appearance to re-identify when reappears\n",
      "  â€¢ Maintains ID through temporary disappearance\n",
      "\n",
      "âœ“ REDUCES ID SWITCHES\n",
      "  â€¢ Appearance features distinguish similar people\n",
      "  â€¢ Less confusion in crowded scenes\n",
      "  â€¢ Better long-term consistency\n",
      "\n",
      "âœ“ REAL-TIME CAPABLE\n",
      "  â€¢ Efficient algorithms (Kalman + Hungarian)\n",
      "  â€¢ ~1-2ms overhead per frame\n",
      "  â€¢ Works with YOLO at 30 FPS\n",
      "\n",
      "âœ“ NO TRAINING NEEDED\n",
      "  â€¢ Uses pre-trained ReID model\n",
      "  â€¢ Works out-of-the-box\n",
      "  â€¢ Just configure parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "DEEPSORT vs ALTERNATIVES:\n",
      "\n",
      "ByteTrack (2021):\n",
      "â”œâ”€â”€ Pro: Simpler, faster, no appearance features\n",
      "â”œâ”€â”€ Pro: Better on high frame-rate videos\n",
      "â””â”€â”€ Con: Less robust to occlusions\n",
      "\n",
      "FairMOT (2020):\n",
      "â”œâ”€â”€ Pro: Joint detection + tracking (end-to-end)\n",
      "â”œâ”€â”€ Pro: Single-shot inference\n",
      "â””â”€â”€ Con: Requires custom training\n",
      "\n",
      "DeepSORT (2017):\n",
      "â”œâ”€â”€ Pro: Best balance of speed and accuracy\n",
      "â”œâ”€â”€ Pro: Proven, stable, widely used\n",
      "â”œâ”€â”€ Pro: Easy to integrate with any detector\n",
      "â””â”€â”€ âœ… BEST FOR OUR USE CASE\n",
      "\n",
      "==================================================\n",
      "\n",
      "PRACTICAL CONSIDERATIONS:\n",
      "\n",
      "Performance:\n",
      "- Detection (YOLO): ~30ms per frame (GPU)\n",
      "- Tracking (DeepSORT): ~1-2ms per frame\n",
      "- Total: ~32ms â†’ ~31 FPS âœ“\n",
      "\n",
      "Tuning Parameters:\n",
      "- max_age: Higher = more robust to occlusions (but more false positives)\n",
      "- min_hits: Higher = fewer false tracks (but slower confirmation)\n",
      "- max_iou_distance: Lower = stricter matching (fewer ID switches)\n",
      "\n",
      "For Security System:\n",
      "âœ“ Perfect for office/factory scenarios\n",
      "âœ“ Handles 5-20 people simultaneously\n",
      "âœ“ Good ID persistence (95%+ accuracy)\n",
      "âœ“ Acceptable performance overhead\n",
      "\n",
      "\n",
      "âœ… Exercise 1.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.1: UNDERSTAND DEEPSORT ALGORITHM\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.1: Understanding DeepSORT Algorithm\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: What is Object Tracking?\n",
    "\n",
    "Object Detection vs Object Tracking:\n",
    "- Detection: Find objects in EACH frame independently\n",
    "  â†’ Frame 1: \"There's a person at (100, 200)\"\n",
    "  â†’ Frame 2: \"There's a person at (105, 210)\"\n",
    "  â†’ No connection between frames\n",
    "  \n",
    "- Tracking: Connect detections across frames\n",
    "  â†’ Frame 1: \"Person ID=1 at (100, 200)\"\n",
    "  â†’ Frame 2: \"Person ID=1 at (105, 210)\"  â† Same person!\n",
    "  â†’ Maintains identity over time\n",
    "\n",
    "Why Tracking Matters for Security:\n",
    "- Count UNIQUE people (not just detections)\n",
    "- Track movement patterns\n",
    "- Detect when someone enters/exits\n",
    "- Maintain identity for face recognition\n",
    "- Detect loitering (time-based behavior)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ” DEEPSORT ALGORITHM BREAKDOWN:\n",
    "\n",
    "DeepSORT = Detection-based Tracking with Deep Learning\n",
    "\n",
    "Original SORT (Simple Online Realtime Tracking):\n",
    "â””â”€â”€ Uses only motion information (bounding box positions)\n",
    "â””â”€â”€ Fast but loses track during occlusions\n",
    "\n",
    "DeepSORT = SORT + Deep Appearance Features:\n",
    "â””â”€â”€ Adds \"how the object looks\" (appearance)\n",
    "â””â”€â”€ Uses ReID (Re-Identification) network\n",
    "â””â”€â”€ Better handles occlusions and crowded scenes\n",
    "â””â”€â”€ Reduces ID switches\n",
    "\n",
    "==================================================\n",
    "\n",
    "DEEPSORT PIPELINE (5 Steps):\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. INPUT: Detections from YOLO                  â”‚\n",
    "â”‚    [x, y, w, h, confidence, class]              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2. KALMAN FILTER (Motion Prediction)            â”‚\n",
    "â”‚    â€¢ Predicts where object will be next frame   â”‚\n",
    "â”‚    â€¢ Based on position + velocity               â”‚\n",
    "â”‚    â€¢ Handles smooth, continuous motion          â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚    State: [x, y, aspect_ratio, height,          â”‚\n",
    "â”‚            vx, vy, va, vh]                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 3. APPEARANCE DESCRIPTOR (Deep Learning)        â”‚\n",
    "â”‚    â€¢ Extract 128D feature vector from crop      â”‚\n",
    "â”‚    â€¢ ReID network (trained on person ReID)      â”‚\n",
    "â”‚    â€¢ Captures \"what object looks like\"          â”‚\n",
    "â”‚    â€¢ Similar people â†’ similar embeddings        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 4. MATCHING (Hungarian Algorithm)               â”‚\n",
    "â”‚    â€¢ Match predictions to new detections        â”‚\n",
    "â”‚    â€¢ Cost = Motion + Appearance                 â”‚\n",
    "â”‚    â€¢ Motion: IOU distance (overlap)             â”‚\n",
    "â”‚    â€¢ Appearance: Cosine distance (embeddings)   â”‚\n",
    "â”‚    â€¢ Assigns same ID to matched objects         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 5. TRACK MANAGEMENT                             â”‚\n",
    "â”‚    â€¢ Create new track for unmatched detection   â”‚\n",
    "â”‚    â€¢ Update existing tracks with new detection  â”‚\n",
    "â”‚    â€¢ Delete tracks missing for max_age frames   â”‚\n",
    "â”‚    â€¢ Confirm tracks after min_hits detections   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "==================================================\n",
    "\n",
    "KEY COMPONENTS EXPLAINED:\n",
    "\n",
    "1. KALMAN FILTER (Motion Model)\n",
    "   Purpose: Predict object location in next frame\n",
    "   \n",
    "   State Variables:\n",
    "   â€¢ Position: (x, y) - center of bounding box\n",
    "   â€¢ Size: (aspect_ratio, height)\n",
    "   â€¢ Velocity: (vx, vy, va, vh) - how fast moving\n",
    "   \n",
    "   How it works:\n",
    "   â€¢ Predict: Use velocity to estimate next position\n",
    "   â€¢ Update: Correct prediction with actual detection\n",
    "   â€¢ Handles: Smooth, linear motion\n",
    "\n",
    "2. REID NETWORK (Appearance Model)\n",
    "   Purpose: Recognize same object even after occlusion\n",
    "   \n",
    "   Architecture: CNN (MobileNet or ResNet)\n",
    "   Input: Cropped person image (128x256)\n",
    "   Output: 128D embedding vector\n",
    "   \n",
    "   Training: Pre-trained on person re-identification\n",
    "   â€¢ Learns to extract discriminative features\n",
    "   â€¢ Same person â†’ similar embeddings\n",
    "   â€¢ Different people â†’ different embeddings\n",
    "\n",
    "3. COST MATRIX (Matching)\n",
    "   Purpose: Decide which detection matches which track\n",
    "   \n",
    "   For each (track, detection) pair:\n",
    "   â€¢ Motion cost = 1 - IOU(predicted_box, detected_box)\n",
    "   â€¢ Appearance cost = 1 - cosine_similarity(embeddings)\n",
    "   â€¢ Total cost = Î»â‚ * motion + Î»â‚‚ * appearance\n",
    "   \n",
    "   Lower cost = better match\n",
    "\n",
    "4. HUNGARIAN ALGORITHM (Assignment)\n",
    "   Purpose: Find optimal track-detection assignments\n",
    "   \n",
    "   Problem: Given cost matrix, assign detections to tracks\n",
    "   Goal: Minimize total cost\n",
    "   Complexity: O(nÂ³)\n",
    "   Result: Each detection assigned to at most one track\n",
    "\n",
    "==================================================\n",
    "\n",
    "TRACK LIFECYCLE:\n",
    "\n",
    "NEW DETECTION (no matching track)\n",
    "    â†“\n",
    "TENTATIVE TRACK (unconfirmed, ID assigned but not shown)\n",
    "    â†“ (appears for min_hits consecutive frames)\n",
    "CONFIRMED TRACK (ID now visible, actively tracked)\n",
    "    â†“ (continues being detected)\n",
    "ACTIVE TRACKING (ID maintained, position updated)\n",
    "    â†“ (object becomes occluded/leaves frame)\n",
    "LOST TRACK (no detection, but kept for max_age frames)\n",
    "    â†“ (reappears: continue | doesn't reappear: delete)\n",
    "DELETED TRACK (removed from system)\n",
    "\n",
    "Key Parameters:\n",
    "- min_hits = 3: Need 3 consecutive detections to confirm\n",
    "- max_age = 30: Keep lost track for 30 frames (1 second @ 30fps)\n",
    "- max_iou_distance = 0.7: Matching threshold\n",
    "\n",
    "==================================================\n",
    "\n",
    "WHY DEEPSORT IS POWERFUL:\n",
    "\n",
    "âœ“ HANDLES OCCLUSIONS\n",
    "  â€¢ Track continues even when person hidden\n",
    "  â€¢ Uses appearance to re-identify when reappears\n",
    "  â€¢ Maintains ID through temporary disappearance\n",
    "\n",
    "âœ“ REDUCES ID SWITCHES\n",
    "  â€¢ Appearance features distinguish similar people\n",
    "  â€¢ Less confusion in crowded scenes\n",
    "  â€¢ Better long-term consistency\n",
    "\n",
    "âœ“ REAL-TIME CAPABLE\n",
    "  â€¢ Efficient algorithms (Kalman + Hungarian)\n",
    "  â€¢ ~1-2ms overhead per frame\n",
    "  â€¢ Works with YOLO at 30 FPS\n",
    "\n",
    "âœ“ NO TRAINING NEEDED\n",
    "  â€¢ Uses pre-trained ReID model\n",
    "  â€¢ Works out-of-the-box\n",
    "  â€¢ Just configure parameters\n",
    "\n",
    "==================================================\n",
    "\n",
    "DEEPSORT vs ALTERNATIVES:\n",
    "\n",
    "ByteTrack (2021):\n",
    "â”œâ”€â”€ Pro: Simpler, faster, no appearance features\n",
    "â”œâ”€â”€ Pro: Better on high frame-rate videos\n",
    "â””â”€â”€ Con: Less robust to occlusions\n",
    "\n",
    "FairMOT (2020):\n",
    "â”œâ”€â”€ Pro: Joint detection + tracking (end-to-end)\n",
    "â”œâ”€â”€ Pro: Single-shot inference\n",
    "â””â”€â”€ Con: Requires custom training\n",
    "\n",
    "DeepSORT (2017):\n",
    "â”œâ”€â”€ Pro: Best balance of speed and accuracy\n",
    "â”œâ”€â”€ Pro: Proven, stable, widely used\n",
    "â”œâ”€â”€ Pro: Easy to integrate with any detector\n",
    "â””â”€â”€ âœ… BEST FOR OUR USE CASE\n",
    "\n",
    "==================================================\n",
    "\n",
    "PRACTICAL CONSIDERATIONS:\n",
    "\n",
    "Performance:\n",
    "- Detection (YOLO): ~30ms per frame (GPU)\n",
    "- Tracking (DeepSORT): ~1-2ms per frame\n",
    "- Total: ~32ms â†’ ~31 FPS âœ“\n",
    "\n",
    "Tuning Parameters:\n",
    "- max_age: Higher = more robust to occlusions (but more false positives)\n",
    "- min_hits: Higher = fewer false tracks (but slower confirmation)\n",
    "- max_iou_distance: Lower = stricter matching (fewer ID switches)\n",
    "\n",
    "For Security System:\n",
    "âœ“ Perfect for office/factory scenarios\n",
    "âœ“ Handles 5-20 people simultaneously\n",
    "âœ“ Good ID persistence (95%+ accuracy)\n",
    "âœ“ Acceptable performance overhead\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 1.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c6b3ec-32ab-4888-8431-435e88304020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.2: Install and Verify DeepSORT\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Verifying DeepSORT installation...\n",
      "âœ… DeepSORT import successful!\n",
      "   Package: deep-sort-realtime\n",
      "   Location: Installed via pip\n",
      "\n",
      "ğŸ§ª Testing DeepSORT initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audrey\\AppData\\Roaming\\Python\\Python313\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DeepSORT initialized successfully!\n",
      "\n",
      "ğŸ“Š Configuration:\n",
      "   â€¢ max_age: 30 frames (~1 second @ 30fps)\n",
      "   â€¢ n_init: 3 (confirm after 3 consecutive detections)\n",
      "   â€¢ max_iou_distance: 0.7 (matching threshold)\n",
      "   â€¢ embedder: MobileNet (fast, CPU-friendly)\n",
      "   â€¢ embedder_gpu: False (using CPU)\n",
      "\n",
      "âœ… Exercise 1.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.2: INSTALL AND VERIFY DEEPSORT\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.2: Install and Verify DeepSORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: DeepSORT Implementation Choices\n",
    "\n",
    "Available Implementations:\n",
    "1. nwojke/deep_sort (Original, TensorFlow)\n",
    "   â””â”€â”€ Pros: Original implementation, well-documented\n",
    "   â””â”€â”€ Cons: TensorFlow 1.x, harder to integrate\n",
    "\n",
    "2. deep-sort-realtime (Python package)\n",
    "   â””â”€â”€ Pros: Easy pip install, PyTorch, up-to-date\n",
    "   â””â”€â”€ Cons: Slightly different API\n",
    "   â””â”€â”€ âœ… WE'LL USE THIS\n",
    "\n",
    "3. yolov5-deepsort (Integrated)\n",
    "   â””â”€â”€ Pros: Pre-integrated with YOLOv5\n",
    "   â””â”€â”€ Cons: Tied to YOLOv5 only\n",
    "\n",
    "We choose deep-sort-realtime because:\n",
    "- Easy installation (pip install)\n",
    "- Works with any detector (YOLO, Faster R-CNN, etc.)\n",
    "- Modern PyTorch implementation\n",
    "- Active maintenance\n",
    "- Good documentation\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ”§ Verifying DeepSORT installation...\")\n",
    "\n",
    "try:\n",
    "    from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "    print(\"âœ… DeepSORT import successful!\")\n",
    "    print(\"   Package: deep-sort-realtime\")\n",
    "    print(\"   Location: Installed via pip\")\n",
    "    \n",
    "    # Test initialization\n",
    "    print(\"\\nğŸ§ª Testing DeepSORT initialization...\")\n",
    "    test_tracker = DeepSort(\n",
    "        max_age=30,\n",
    "        n_init=3,\n",
    "        max_iou_distance=0.7,\n",
    "        embedder=\"mobilenet\",  # Fast embedder\n",
    "        embedder_gpu=False      # CPU mode (set True if GPU)\n",
    "    )\n",
    "    print(\"âœ… DeepSORT initialized successfully!\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Configuration:\")\n",
    "    print(f\"   â€¢ max_age: 30 frames (~1 second @ 30fps)\")\n",
    "    print(f\"   â€¢ n_init: 3 (confirm after 3 consecutive detections)\")\n",
    "    print(f\"   â€¢ max_iou_distance: 0.7 (matching threshold)\")\n",
    "    print(f\"   â€¢ embedder: MobileNet (fast, CPU-friendly)\")\n",
    "    print(f\"   â€¢ embedder_gpu: False (using CPU)\")\n",
    "    \n",
    "    del test_tracker\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ DeepSORT import failed: {e}\")\n",
    "    print(\"\\nğŸ“Œ Fix: Run this command:\")\n",
    "    print(\"   pip install deep-sort-realtime\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nâœ… Exercise 1.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db868525-ef78-4a6a-9ebb-348455cf695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 1.3: Load YOLO Detection Model\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Loading YOLOv8n model...\n",
      "â±ï¸  First time: Downloads weights (~6 MB)\n",
      "âœ… YOLOv8n loaded successfully!\n",
      "\n",
      "ğŸ“Š Model Information:\n",
      "   â€¢ Model: YOLOv8n (Nano - Fastest)\n",
      "   â€¢ Input Size: 640x640\n",
      "   â€¢ Classes: 80 (COCO dataset)\n",
      "   â€¢ Person Class ID: 0\n",
      "   â€¢ Expected FPS: 30-50 (GPU), 10-15 (CPU)\n",
      "\n",
      "ğŸ” Testing detection on sample image...\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "âœ… Detection test successful!\n",
      "   â€¢ Test image: bus.jpg\n",
      "   â€¢ People detected: 3\n",
      "   â€¢ Confidence threshold: 0.5\n",
      "\n",
      "âœ… Exercise 1.3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 1.3: LOAD YOLO DETECTION MODEL\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 1.3: Load YOLO Detection Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Why We Need YOLO + DeepSORT\n",
    "\n",
    "YOLO's Role:\n",
    "- Detects people in each frame\n",
    "- Provides bounding boxes [x, y, w, h]\n",
    "- Gives confidence scores\n",
    "- Fast inference (30-50 FPS)\n",
    "\n",
    "DeepSORT's Role:\n",
    "- Takes YOLO detections as input\n",
    "- Assigns unique IDs to each person\n",
    "- Tracks across frames\n",
    "- Maintains IDs through occlusions\n",
    "\n",
    "Integration Flow:\n",
    "Frame â†’ YOLO â†’ Detections â†’ DeepSORT â†’ Tracked Objects with IDs\n",
    "\n",
    "Why YOLOv8n (Nano)?\n",
    "- Fastest YOLO variant (80+ FPS on GPU)\n",
    "- Good accuracy (37.3 mAP)\n",
    "- Perfect for real-time tracking\n",
    "- Low memory footprint\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ”§ Loading YOLOv8n model...\")\n",
    "print(\"â±ï¸  First time: Downloads weights (~6 MB)\")\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "print(\"âœ… YOLOv8n loaded successfully!\")\n",
    "\n",
    "print(\"\\nğŸ“Š Model Information:\")\n",
    "print(\"   â€¢ Model: YOLOv8n (Nano - Fastest)\")\n",
    "print(\"   â€¢ Input Size: 640x640\")\n",
    "print(\"   â€¢ Classes: 80 (COCO dataset)\")\n",
    "print(\"   â€¢ Person Class ID: 0\")\n",
    "print(\"   â€¢ Expected FPS: 30-50 (GPU), 10-15 (CPU)\")\n",
    "\n",
    "print(\"\\nğŸ” Testing detection on sample image...\")\n",
    "\n",
    "# Test on sample image\n",
    "test_url = 'https://ultralytics.com/images/bus.jpg'\n",
    "results = model.predict(test_url, conf=0.5, classes=[0], verbose=False)\n",
    "\n",
    "num_people = len(results[0].boxes)\n",
    "print(f\"âœ… Detection test successful!\")\n",
    "print(f\"   â€¢ Test image: bus.jpg\")\n",
    "print(f\"   â€¢ People detected: {num_people}\")\n",
    "print(f\"   â€¢ Confidence threshold: 0.5\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 1.3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b5ace4-7e67-4680-aca3-0501274e16a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”— PART 2: YOLO + DEEPSORT INTEGRATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”— PART 2: YOLO + DEEPSORT INTEGRATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779077ab-5bc2-4d5a-8174-7da40070098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.1: Build YOLO + DeepSORT Integration Pipeline\n",
      "================================================================================\n",
      "\n",
      "âœ… Format conversion function created!\n",
      "   â€¢ Function: yolo_to_deepsort()\n",
      "   â€¢ Input: YOLO boxes\n",
      "   â€¢ Output: DeepSORT format detections\n",
      "\n",
      "ğŸ§ª Testing conversion...\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "âœ… Conversion successful!\n",
      "   â€¢ YOLO detections: 3\n",
      "   â€¢ Converted detections: 3\n",
      "\n",
      "   Example detection format:\n",
      "   ([np.float32(48.550438), np.float32(398.5523), np.float32(196.79517), np.float32(504.1504)], 0.8656910061836243, 'person')\n",
      "\n",
      "âœ… Exercise 2.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.1: BUILD INTEGRATION PIPELINE\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.1: Build YOLO + DeepSORT Integration Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Integration Architecture\n",
    "\n",
    "The Challenge:\n",
    "- YOLO outputs: [x1, y1, x2, y2, conf, class_id]\n",
    "- DeepSORT expects: [[x, y, w, h], conf, class_name]\n",
    "- Need to convert format!\n",
    "\n",
    "Pipeline Steps:\n",
    "1. Run YOLO detection on frame\n",
    "2. Convert YOLO format to DeepSORT format\n",
    "3. Update tracker with detections\n",
    "4. Get tracked objects with IDs\n",
    "5. Visualize results\n",
    "\n",
    "Format Conversion:\n",
    "YOLO: (x1, y1, x2, y2) = top-left + bottom-right corners\n",
    "DeepSORT: (x, y, w, h) = top-left corner + width/height\n",
    "\n",
    "Conversion:\n",
    "- x = x1\n",
    "- y = y1\n",
    "- w = x2 - x1\n",
    "- h = y2 - y1\n",
    "\"\"\"\n",
    "\n",
    "def yolo_to_deepsort(yolo_detections):\n",
    "    \"\"\"\n",
    "    Convert YOLO detections to DeepSORT format\n",
    "    \n",
    "    Args:\n",
    "        yolo_detections: YOLO boxes object\n",
    "        \n",
    "    Returns:\n",
    "        List of detections in format: [[x, y, w, h], confidence, 'person']\n",
    "    \"\"\"\n",
    "    deepsort_detections = []\n",
    "    \n",
    "    for box in yolo_detections:\n",
    "        # Get coordinates\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "        conf = float(box.conf[0])\n",
    "        \n",
    "        # Convert to DeepSORT format\n",
    "        x = x1\n",
    "        y = y1\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        \n",
    "        # Append in DeepSORT format\n",
    "        deepsort_detections.append(([x, y, w, h], conf, 'person'))\n",
    "    \n",
    "    return deepsort_detections\n",
    "\n",
    "print(\"\\nâœ… Format conversion function created!\")\n",
    "print(\"   â€¢ Function: yolo_to_deepsort()\")\n",
    "print(\"   â€¢ Input: YOLO boxes\")\n",
    "print(\"   â€¢ Output: DeepSORT format detections\")\n",
    "\n",
    "print(\"\\nğŸ§ª Testing conversion...\")\n",
    "\n",
    "# Test conversion\n",
    "test_results = model.predict(test_url, conf=0.5, classes=[0], verbose=False)\n",
    "test_detections = test_results[0].boxes\n",
    "\n",
    "converted = yolo_to_deepsort(test_detections)\n",
    "\n",
    "print(f\"âœ… Conversion successful!\")\n",
    "print(f\"   â€¢ YOLO detections: {len(test_detections)}\")\n",
    "print(f\"   â€¢ Converted detections: {len(converted)}\")\n",
    "print(f\"\\n   Example detection format:\")\n",
    "print(f\"   {converted[0]}\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 2.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb3f41d-3e78-4aff-8d6e-9efb3f513e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.2: First Tracking Test on Single Image\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Initializing tracker...\n",
      "âœ… Tracker initialized\n",
      "\n",
      "ğŸ¯ Running detection + tracking pipeline...\n",
      "\n",
      "   Step 1: YOLO Detection...\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "   âœ… Detected 3 people\n",
      "\n",
      "   Step 2: Format Conversion...\n",
      "   âœ… Converted 3 detections\n",
      "\n",
      "   Step 3: Update Tracker...\n",
      "   âœ… Tracker updated\n",
      "\n",
      "   Step 4: Get Confirmed Tracks...\n",
      "   âœ… Confirmed tracks: 0\n",
      "   âš ï¸  Note: May be 0 (needs n_init=3 consecutive frames)\n",
      "\n",
      "ğŸ“Š Track Status:\n",
      "   â€¢ Total tracks: 3\n",
      "   â€¢ Confirmed tracks: 0\n",
      "   â€¢ Tentative tracks: 3\n",
      "\n",
      "ğŸ“‹ Track Details:\n",
      "   Track 1: ID=1, â§— Tentative\n",
      "   Track 2: ID=2, â§— Tentative\n",
      "   Track 3: ID=3, â§— Tentative\n",
      "\n",
      "ğŸ’¡ Key Insight:\n",
      "   â€¢ Single image â†’ Tracks may be 'tentative' (not confirmed)\n",
      "   â€¢ Need 3 consecutive frames (n_init=3) to confirm track\n",
      "   â€¢ Next: Test on video for real tracking!\n",
      "\n",
      "âœ… Exercise 2.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.2: FIRST TRACKING TEST\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.2: First Tracking Test on Single Image\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Testing Tracking Pipeline\n",
    "\n",
    "Why Test on Single Image First?\n",
    "- Verifies integration works\n",
    "- Checks data flow\n",
    "- Easier to debug than video\n",
    "- Fast feedback loop\n",
    "\n",
    "Note: Tracking on single image isn't meaningful\n",
    "- Tracks need multiple frames to persist\n",
    "- IDs may not be confirmed (need n_init frames)\n",
    "- But it validates the pipeline!\n",
    "\n",
    "Next Step: Test on video (Exercise 2.3)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ”§ Initializing tracker...\")\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,\n",
    "    n_init=3,\n",
    "    max_iou_distance=0.7,\n",
    "    embedder=\"mobilenet\",\n",
    "    embedder_gpu=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Tracker initialized\")\n",
    "\n",
    "print(\"\\nğŸ¯ Running detection + tracking pipeline...\")\n",
    "\n",
    "# Step 1: YOLO Detection\n",
    "print(\"\\n   Step 1: YOLO Detection...\")\n",
    "results = model.predict(test_url, conf=0.5, classes=[0], verbose=False)\n",
    "detections = results[0].boxes\n",
    "print(f\"   âœ… Detected {len(detections)} people\")\n",
    "\n",
    "# Step 2: Format conversion\n",
    "print(\"\\n   Step 2: Format Conversion...\")\n",
    "deepsort_input = yolo_to_deepsort(detections)\n",
    "print(f\"   âœ… Converted {len(deepsort_input)} detections\")\n",
    "\n",
    "# Step 3: Update tracker\n",
    "print(\"\\n   Step 3: Update Tracker...\")\n",
    "frame = results[0].orig_img\n",
    "tracks = tracker.update_tracks(deepsort_input, frame=frame)\n",
    "print(f\"   âœ… Tracker updated\")\n",
    "\n",
    "# Step 4: Get confirmed tracks\n",
    "print(\"\\n   Step 4: Get Confirmed Tracks...\")\n",
    "confirmed_tracks = [track for track in tracks if track.is_confirmed()]\n",
    "print(f\"   âœ… Confirmed tracks: {len(confirmed_tracks)}\")\n",
    "print(f\"   âš ï¸  Note: May be 0 (needs n_init=3 consecutive frames)\")\n",
    "\n",
    "# Show all tracks (including tentative)\n",
    "print(f\"\\nğŸ“Š Track Status:\")\n",
    "print(f\"   â€¢ Total tracks: {len(tracks)}\")\n",
    "print(f\"   â€¢ Confirmed tracks: {len(confirmed_tracks)}\")\n",
    "print(f\"   â€¢ Tentative tracks: {len(tracks) - len(confirmed_tracks)}\")\n",
    "\n",
    "if len(tracks) > 0:\n",
    "    print(f\"\\nğŸ“‹ Track Details:\")\n",
    "    for i, track in enumerate(tracks):\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()\n",
    "        is_confirmed = \"âœ“ Confirmed\" if track.is_confirmed() else \"â§— Tentative\"\n",
    "        print(f\"   Track {i+1}: ID={track_id}, {is_confirmed}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"   â€¢ Single image â†’ Tracks may be 'tentative' (not confirmed)\")\n",
    "print(\"   â€¢ Need 3 consecutive frames (n_init=3) to confirm track\")\n",
    "print(\"   â€¢ Next: Test on video for real tracking!\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 2.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08e020d-d1db-4a9f-ba84-066660ddbe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 2.3: Video Tracking Demo with Webcam\n",
      "================================================================================\n",
      "\n",
      "ğŸ¥ WEBCAM TRACKING CODE\n",
      "\n",
      "Below is the complete code for real-time webcam tracking.\n",
      "This demonstrates the full YOLO + DeepSORT pipeline!\n",
      "\n",
      "Features:\n",
      "âœ“ Real-time detection + tracking\n",
      "âœ“ Unique IDs for each person\n",
      "âœ“ FPS counter\n",
      "âœ“ Active track count\n",
      "âœ“ Total unique people counter\n",
      "âœ“ Press 'q' to quit\n",
      "\n",
      "ğŸ“ To run this demo:\n",
      "1. Copy the code below to a new cell\n",
      "2. Execute the cell\n",
      "3. Your webcam will open\n",
      "4. Walk around to test tracking!\n",
      "5. Press 'q' to quit\n",
      "\n",
      "âš ï¸  Note: Requires webcam. If no webcam, skip to next exercise.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CODE: WEBCAM TRACKING DEMO\n",
      "================================================================================\n",
      "\n",
      "Copy this code to a new cell to run webcam tracking:\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "import cv2\n",
      "from ultralytics import YOLO\n",
      "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
      "import time\n",
      "\n",
      "# Load models\n",
      "print(\"Loading models...\")\n",
      "model = YOLO('yolov8n.pt')\n",
      "tracker = DeepSort(max_age=30, n_init=3, embedder=\"mobilenet\", embedder_gpu=False)\n",
      "\n",
      "# Open webcam\n",
      "cap = cv2.VideoCapture(0)\n",
      "\n",
      "if not cap.isOpened():\n",
      "    print(\"âŒ Cannot open webcam!\")\n",
      "else:\n",
      "    print(\"âœ… Webcam opened successfully!\")\n",
      "    print(\"ğŸ¥ Starting tracking... (Press 'q' to quit)\")\n",
      "\n",
      "    # FPS tracking\n",
      "    fps_counter = 0\n",
      "    start_time = time.time()\n",
      "    fps = 0\n",
      "\n",
      "    # Track unique IDs\n",
      "    unique_ids = set()\n",
      "\n",
      "    while True:\n",
      "        ret, frame = cap.read()\n",
      "        if not ret:\n",
      "            break\n",
      "\n",
      "        # 1. YOLO Detection\n",
      "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
      "        detections = results[0].boxes\n",
      "\n",
      "        # 2. Convert to DeepSORT format\n",
      "        deepsort_input = []\n",
      "        for box in detections:\n",
      "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
      "            conf = float(box.conf[0])\n",
      "            w = x2 - x1\n",
      "            h = y2 - y1\n",
      "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
      "\n",
      "        # 3. Update tracker\n",
      "        tracks = tracker.update_tracks(deepsort_input, frame=frame)\n",
      "\n",
      "        # 4. Visualize tracks\n",
      "        for track in tracks:\n",
      "            if not track.is_confirmed():\n",
      "                continue\n",
      "\n",
      "            track_id = track.track_id\n",
      "            unique_ids.add(track_id)\n",
      "            bbox = track.to_ltrb()\n",
      "            x1, y1, x2, y2 = map(int, bbox)\n",
      "\n",
      "            # Draw bounding box\n",
      "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
      "\n",
      "            # Draw ID\n",
      "            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
      "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
      "\n",
      "        # 5. Calculate FPS\n",
      "        fps_counter += 1\n",
      "        if fps_counter % 30 == 0:\n",
      "            elapsed = time.time() - start_time\n",
      "            fps = 30 / elapsed\n",
      "            start_time = time.time()\n",
      "\n",
      "        # 6. Display info\n",
      "        cv2.putText(frame, f'FPS: {fps:.1f}', (10, 30),\n",
      "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
      "        cv2.putText(frame, f'Tracking: {len([t for t in tracks if t.is_confirmed()])} people',\n",
      "                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
      "        cv2.putText(frame, f'Total Unique: {len(unique_ids)}',\n",
      "                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
      "\n",
      "        # 7. Show frame\n",
      "        cv2.imshow('Security System - DeepSORT Tracking', frame)\n",
      "\n",
      "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "            break\n",
      "\n",
      "    cap.release()\n",
      "    cv2.destroyAllWindows()\n",
      "\n",
      "    print(f\"Tracking demo complete!\")\n",
      "    print(f\"Total unique people tracked: {len(unique_ids)}\")\n",
      "    print(f\"Average FPS: {fps:.1f}\")\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "ğŸ’¡ What to Observe:\n",
      "   â€¢ Each person gets a unique ID (1, 2, 3...)\n",
      "   â€¢ IDs persist as person moves\n",
      "   â€¢ IDs maintained even if person briefly hidden\n",
      "   â€¢ FPS shows real-time performance\n",
      "   â€¢ Total unique counts everyone seen\n",
      "\n",
      "âœ… Exercise 2.3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 2.3: VIDEO TRACKING DEMO\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 2.3: Video Tracking Demo with Webcam\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Real-Time Video Tracking\n",
    "\n",
    "Video Tracking Pipeline:\n",
    "1. Capture frame from video source\n",
    "2. Detect people with YOLO\n",
    "3. Convert to DeepSORT format\n",
    "4. Update tracker\n",
    "5. Draw tracking visualization\n",
    "6. Calculate FPS\n",
    "7. Display frame\n",
    "8. Repeat for next frame\n",
    "\n",
    "Performance Considerations:\n",
    "- Detection: ~30ms per frame (GPU), ~100ms (CPU)\n",
    "- Tracking: ~1-2ms per frame\n",
    "- Visualization: ~5-10ms per frame\n",
    "- Target: 30 FPS = 33ms per frame\n",
    "\n",
    "Real-time Requirements:\n",
    "- Process frames faster than they arrive\n",
    "- Maintain smooth video playback\n",
    "- Update UI without lag\n",
    "\n",
    "Video Sources:\n",
    "- Webcam (cv2.VideoCapture(0))\n",
    "- Video file (cv2.VideoCapture('video.mp4'))\n",
    "- RTSP stream (cv2.VideoCapture('rtsp://...'))\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¥ WEBCAM TRACKING CODE\n",
    "\n",
    "Below is the complete code for real-time webcam tracking.\n",
    "This demonstrates the full YOLO + DeepSORT pipeline!\n",
    "\n",
    "Features:\n",
    "âœ“ Real-time detection + tracking\n",
    "âœ“ Unique IDs for each person\n",
    "âœ“ FPS counter\n",
    "âœ“ Active track count\n",
    "âœ“ Total unique people counter\n",
    "âœ“ Press 'q' to quit\n",
    "\n",
    "ğŸ“ To run this demo:\n",
    "1. Copy the code below to a new cell\n",
    "2. Execute the cell\n",
    "3. Your webcam will open\n",
    "4. Walk around to test tracking!\n",
    "5. Press 'q' to quit\n",
    "\n",
    "âš ï¸  Note: Requires webcam. If no webcam, skip to next exercise.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CODE: WEBCAM TRACKING DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Copy this code to a new cell to run webcam tracking:\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "model = YOLO('yolov8n.pt')\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder=\"mobilenet\", embedder_gpu=False)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Cannot open webcam!\")\n",
    "else:\n",
    "    print(\"âœ… Webcam opened successfully!\")\n",
    "    print(\"ğŸ¥ Starting tracking... (Press 'q' to quit)\")\n",
    "    \n",
    "    # FPS tracking\n",
    "    fps_counter = 0\n",
    "    start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    # Track unique IDs\n",
    "    unique_ids = set()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. YOLO Detection\n",
    "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        # 2. Convert to DeepSORT format\n",
    "        deepsort_input = []\n",
    "        for box in detections:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0])\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            deepsort_input.append(([x1, y1, w, h], conf, 'person'))\n",
    "        \n",
    "        # 3. Update tracker\n",
    "        tracks = tracker.update_tracks(deepsort_input, frame=frame)\n",
    "        \n",
    "        # 4. Visualize tracks\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            \n",
    "            track_id = track.track_id\n",
    "            unique_ids.add(track_id)\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw ID\n",
    "            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # 5. Calculate FPS\n",
    "        fps_counter += 1\n",
    "        if fps_counter % 30 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            fps = 30 / elapsed\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # 6. Display info\n",
    "        cv2.putText(frame, f'FPS: {fps:.1f}', (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f'Tracking: {len([t for t in tracks if t.is_confirmed()])} people',\n",
    "                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f'Total Unique: {len(unique_ids)}',\n",
    "                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # 7. Show frame\n",
    "        cv2.imshow('Security System - DeepSORT Tracking', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"Tracking demo complete!\")\n",
    "    print(f\"Total unique people tracked: {len(unique_ids)}\")\n",
    "    print(f\"Average FPS: {fps:.1f}\")\n",
    "-----------------------------------------------------------------------\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ’¡ What to Observe:\")\n",
    "print(\"   â€¢ Each person gets a unique ID (1, 2, 3...)\")\n",
    "print(\"   â€¢ IDs persist as person moves\")\n",
    "print(\"   â€¢ IDs maintained even if person briefly hidden\")\n",
    "print(\"   â€¢ FPS shows real-time performance\")\n",
    "print(\"   â€¢ Total unique counts everyone seen\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 2.3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713709b1-c29a-4ee2-b890-63251f90a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¨ PART 3: VISUALIZATION & PERFORMANCE TESTING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¨ PART 3: VISUALIZATION & PERFORMANCE TESTING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e3ca8c-d9a8-4a01-aff4-dac8eebe676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 3.1: Create Advanced Visualization Functions\n",
      "================================================================================\n",
      "âœ… Function created: get_color()\n",
      "   â€¢ Purpose: Consistent colors for track IDs\n",
      "   â€¢ Colors available: 12\n",
      "\n",
      "âœ… Function created: draw_tracks()\n",
      "   â€¢ Purpose: Draw bounding boxes + IDs + trajectories\n",
      "   â€¢ Features:\n",
      "      - Colored bounding boxes per ID\n",
      "      - ID label with background\n",
      "      - Movement trajectory (last 30 points)\n",
      "      - Fading effect on older trajectory points\n",
      "\n",
      "âœ… Function created: draw_info_panel()\n",
      "   â€¢ Purpose: Display performance metrics\n",
      "   â€¢ Shows: FPS, active tracks, total unique\n",
      "\n",
      "ğŸ“Š Visualization Toolkit Ready:\n",
      "   âœ… get_color() - Consistent ID colors\n",
      "   âœ… draw_tracks() - Boxes + IDs + trajectories\n",
      "   âœ… draw_info_panel() - Performance metrics\n",
      "\n",
      "âœ… Exercise 3.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 3.1: CREATE ADVANCED VISUALIZATION FUNCTIONS\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 3.1: Create Advanced Visualization Functions\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Effective Tracking Visualization\n",
    "\n",
    "Good Visualization Shows:\n",
    "1. Bounding boxes around tracked objects\n",
    "2. Unique ID for each track\n",
    "3. Track trajectories (path traveled)\n",
    "4. Performance metrics (FPS, count)\n",
    "5. System status\n",
    "\n",
    "Design Principles:\n",
    "âœ“ Clear, readable text (good contrast)\n",
    "âœ“ Consistent colors per ID (easier to follow)\n",
    "âœ“ Non-intrusive overlays (don't block view)\n",
    "âœ“ Smooth trajectories (show movement)\n",
    "âœ“ Performance info visible but not distracting\n",
    "\n",
    "Color Coding:\n",
    "- Different color for each track ID\n",
    "- Helps distinguish multiple people\n",
    "- Color persists with ID across frames\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Color palette for track IDs\n",
    "COLORS = [\n",
    "    (255, 0, 0),    # Red\n",
    "    (0, 255, 0),    # Green\n",
    "    (0, 0, 255),    # Blue\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (128, 0, 128),  # Purple\n",
    "    (255, 128, 0),  # Orange\n",
    "    (0, 128, 255),  # Light Blue\n",
    "    (128, 255, 0),  # Lime\n",
    "    (255, 0, 128),  # Pink\n",
    "    (128, 255, 255) # Light Cyan\n",
    "]\n",
    "\n",
    "def get_color(track_id):\n",
    "    \"\"\"\n",
    "    Get consistent color for a track ID\n",
    "    \n",
    "    Args:\n",
    "        track_id: Integer track ID\n",
    "        \n",
    "    Returns:\n",
    "        BGR color tuple\n",
    "    \"\"\"\n",
    "    return COLORS[track_id % len(COLORS)]\n",
    "\n",
    "print(\"âœ… Function created: get_color()\")\n",
    "print(\"   â€¢ Purpose: Consistent colors for track IDs\")\n",
    "print(\"   â€¢ Colors available: 12\")\n",
    "\n",
    "def draw_tracks(frame, tracks, trajectories=None, show_trajectory=True):\n",
    "    \"\"\"\n",
    "    Draw tracking results on frame with trajectories\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (BGR)\n",
    "        tracks: List of tracks from DeepSORT\n",
    "        trajectories: Dict of track_id -> list of (x, y) centers\n",
    "        show_trajectory: Whether to draw movement trails\n",
    "        \n",
    "    Returns:\n",
    "        Annotated frame, updated trajectories\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()\n",
    "    \n",
    "    if trajectories is None:\n",
    "        trajectories = defaultdict(list)\n",
    "    \n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        \n",
    "        # Get color for this ID\n",
    "        color = get_color(track_id)\n",
    "        \n",
    "        # Calculate center point\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        \n",
    "        # Update trajectory\n",
    "        trajectories[track_id].append((center_x, center_y))\n",
    "        \n",
    "        # Keep only last 30 points (1 second @ 30fps)\n",
    "        if len(trajectories[track_id]) > 30:\n",
    "            trajectories[track_id] = trajectories[track_id][-30:]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 3)\n",
    "        \n",
    "        # Draw ID label with background\n",
    "        label = f'ID: {track_id}'\n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        label_w, label_h = label_size\n",
    "        \n",
    "        # Background rectangle for text\n",
    "        cv2.rectangle(annotated, (x1, y1 - label_h - 10), \n",
    "                     (x1 + label_w + 10, y1), color, -1)\n",
    "        \n",
    "        # Text (white on colored background)\n",
    "        cv2.putText(annotated, label, (x1 + 5, y1 - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw trajectory (path traveled)\n",
    "        if show_trajectory and len(trajectories[track_id]) > 1:\n",
    "            points = trajectories[track_id]\n",
    "            for i in range(len(points) - 1):\n",
    "                # Gradually fade older points (alpha effect)\n",
    "                alpha = (i + 1) / len(points)\n",
    "                thickness = max(1, int(3 * alpha))\n",
    "                cv2.line(annotated, points[i], points[i + 1], color, thickness)\n",
    "            \n",
    "            # Draw circle at current position\n",
    "            cv2.circle(annotated, (center_x, center_y), 5, color, -1)\n",
    "    \n",
    "    return annotated, trajectories\n",
    "\n",
    "print(\"\\nâœ… Function created: draw_tracks()\")\n",
    "print(\"   â€¢ Purpose: Draw bounding boxes + IDs + trajectories\")\n",
    "print(\"   â€¢ Features:\")\n",
    "print(\"      - Colored bounding boxes per ID\")\n",
    "print(\"      - ID label with background\")\n",
    "print(\"      - Movement trajectory (last 30 points)\")\n",
    "print(\"      - Fading effect on older trajectory points\")\n",
    "\n",
    "def draw_info_panel(frame, fps, active_tracks, total_unique_ids):\n",
    "    \"\"\"\n",
    "    Draw information panel on frame\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame\n",
    "        fps: Current FPS\n",
    "        active_tracks: Number of active tracks\n",
    "        total_unique_ids: Total unique people seen\n",
    "        \n",
    "    Returns:\n",
    "        Annotated frame\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()\n",
    "    \n",
    "    # Semi-transparent overlay for panel\n",
    "    overlay = annotated.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (400, 160), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.6, annotated, 0.4, 0, annotated)\n",
    "    \n",
    "    # Draw text\n",
    "    cv2.putText(annotated, f'FPS: {fps:.1f}', (20, 45),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated, f'Active Tracks: {active_tracks}', (20, 85),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated, f'Total Unique: {total_unique_ids}', (20, 120),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated, 'Press Q to quit', (20, 150),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    \n",
    "    return annotated\n",
    "\n",
    "print(\"\\nâœ… Function created: draw_info_panel()\")\n",
    "print(\"   â€¢ Purpose: Display performance metrics\")\n",
    "print(\"   â€¢ Shows: FPS, active tracks, total unique\")\n",
    "\n",
    "print(\"\\nğŸ“Š Visualization Toolkit Ready:\")\n",
    "print(\"   âœ… get_color() - Consistent ID colors\")\n",
    "print(\"   âœ… draw_tracks() - Boxes + IDs + trajectories\")\n",
    "print(\"   âœ… draw_info_panel() - Performance metrics\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 3.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c6c0417-cfce-493b-a1f5-bf1f949ccbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 3.2: Benchmark Detection + Tracking Performance\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ Setting up benchmark...\n",
      "âœ… Benchmark setup complete\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK 1: YOLO Detection Only\n",
      "================================================================================\n",
      "âœ… YOLO Detection Performance:\n",
      "   â€¢ Frames processed: 30\n",
      "   â€¢ Total time: 1.12s\n",
      "   â€¢ Average FPS: 26.9\n",
      "   â€¢ Time per frame: 37.2ms\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK 2: YOLO + DeepSORT Tracking\n",
      "================================================================================\n",
      "âœ… YOLO + DeepSORT Performance:\n",
      "   â€¢ Frames processed: 30\n",
      "   â€¢ Total time: 1.11s\n",
      "   â€¢ Average FPS: 27.1\n",
      "   â€¢ Time per frame: 36.9ms\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Comparison:\n",
      "   â€¢ YOLO only: 26.9 FPS\n",
      "   â€¢ YOLO + DeepSORT: 27.1 FPS\n",
      "   â€¢ Tracking overhead: -0.2 FPS (-0.7%)\n",
      "   â€¢ Time added by tracking: -0.2ms\n",
      "\n",
      "ğŸ¯ Performance Assessment:\n",
      "   âœ“ GOOD: 27.1 FPS is acceptable for most applications\n",
      "   â€¢ Slightly below 30 FPS target\n",
      "   â€¢ Still usable for security system\n",
      "\n",
      "ğŸ’¡ Optimization Tips:\n",
      "   â€¢ Reduce resolution: 1280x720 â†’ 640x480 (2x faster)\n",
      "   â€¢ Use GPU: Enable embedder_gpu=True (3-5x faster)\n",
      "   â€¢ Skip frames: Process every 2nd frame (2x faster)\n",
      "   â€¢ Lower confidence: conf=0.6 â†’ 0.7 (fewer detections)\n",
      "\n",
      "âœ… Exercise 3.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 3.2: PERFORMANCE BENCHMARKING\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 3.2: Benchmark Detection + Tracking Performance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“– THEORY: Performance Analysis\n",
    "\n",
    "Key Metrics:\n",
    "1. Detection FPS (YOLO only)\n",
    "2. Tracking FPS (YOLO + DeepSORT)\n",
    "3. Overhead (tracking cost)\n",
    "4. Frame processing time\n",
    "\n",
    "Performance Breakdown:\n",
    "- YOLO Detection: ~30ms (GPU), ~100ms (CPU)\n",
    "- Format Conversion: <1ms\n",
    "- DeepSORT Update: ~1-2ms\n",
    "- Visualization: ~5-10ms\n",
    "- Total: ~36ms â†’ ~28 FPS (GPU)\n",
    "\n",
    "Target: 30 FPS (33ms per frame)\n",
    "\n",
    "Factors Affecting Performance:\n",
    "- Number of detected objects\n",
    "- Image resolution\n",
    "- GPU vs CPU\n",
    "- Tracking parameters (max_age, n_init)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ”§ Setting up benchmark...\")\n",
    "\n",
    "# Create dummy frame\n",
    "dummy_frame = np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8)\n",
    "\n",
    "# Initialize fresh tracker\n",
    "benchmark_tracker = DeepSort(\n",
    "    max_age=30,\n",
    "    n_init=3,\n",
    "    embedder=\"mobilenet\",\n",
    "    embedder_gpu=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Benchmark setup complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK 1: YOLO Detection Only\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "num_frames = 30\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(num_frames):\n",
    "    results = model.predict(dummy_frame, conf=0.5, classes=[0], verbose=False)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "yolo_fps = num_frames / elapsed\n",
    "\n",
    "print(f\"âœ… YOLO Detection Performance:\")\n",
    "print(f\"   â€¢ Frames processed: {num_frames}\")\n",
    "print(f\"   â€¢ Total time: {elapsed:.2f}s\")\n",
    "print(f\"   â€¢ Average FPS: {yolo_fps:.1f}\")\n",
    "print(f\"   â€¢ Time per frame: {(elapsed/num_frames)*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK 2: YOLO + DeepSORT Tracking\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(num_frames):\n",
    "    # Detection\n",
    "    results = model.predict(dummy_frame, conf=0.5, classes=[0], verbose=False)\n",
    "    detections = results[0].boxes\n",
    "    \n",
    "    # Convert\n",
    "    deepsort_input = yolo_to_deepsort(detections)\n",
    "    \n",
    "    # Track\n",
    "    tracks = benchmark_tracker.update_tracks(deepsort_input, frame=dummy_frame)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "tracking_fps = num_frames / elapsed\n",
    "\n",
    "print(f\"âœ… YOLO + DeepSORT Performance:\")\n",
    "print(f\"   â€¢ Frames processed: {num_frames}\")\n",
    "print(f\"   â€¢ Total time: {elapsed:.2f}s\")\n",
    "print(f\"   â€¢ Average FPS: {tracking_fps:.1f}\")\n",
    "print(f\"   â€¢ Time per frame: {(elapsed/num_frames)*1000:.1f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "overhead = yolo_fps - tracking_fps\n",
    "overhead_pct = (overhead / yolo_fps) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Comparison:\")\n",
    "print(f\"   â€¢ YOLO only: {yolo_fps:.1f} FPS\")\n",
    "print(f\"   â€¢ YOLO + DeepSORT: {tracking_fps:.1f} FPS\")\n",
    "print(f\"   â€¢ Tracking overhead: {overhead:.1f} FPS ({overhead_pct:.1f}%)\")\n",
    "print(f\"   â€¢ Time added by tracking: {((1/tracking_fps - 1/yolo_fps)*1000):.1f}ms\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Performance Assessment:\")\n",
    "if tracking_fps >= 30:\n",
    "    print(f\"   âœ… EXCELLENT: {tracking_fps:.1f} FPS exceeds 30 FPS target!\")\n",
    "    print(f\"   â€¢ Real-time tracking achieved\")\n",
    "    print(f\"   â€¢ Smooth video playback\")\n",
    "elif tracking_fps >= 20:\n",
    "    print(f\"   âœ“ GOOD: {tracking_fps:.1f} FPS is acceptable for most applications\")\n",
    "    print(f\"   â€¢ Slightly below 30 FPS target\")\n",
    "    print(f\"   â€¢ Still usable for security system\")\n",
    "elif tracking_fps >= 15:\n",
    "    print(f\"   âš  MODERATE: {tracking_fps:.1f} FPS may feel choppy\")\n",
    "    print(f\"   â€¢ Consider reducing resolution\")\n",
    "    print(f\"   â€¢ Or skip frames (process every 2nd frame)\")\n",
    "else:\n",
    "    print(f\"   âŒ LOW: {tracking_fps:.1f} FPS too slow for real-time\")\n",
    "    print(f\"   â€¢ Need GPU acceleration\")\n",
    "    print(f\"   â€¢ Or significant optimization required\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Optimization Tips:\")\n",
    "print(f\"   â€¢ Reduce resolution: 1280x720 â†’ 640x480 (2x faster)\")\n",
    "print(f\"   â€¢ Use GPU: Enable embedder_gpu=True (3-5x faster)\")\n",
    "print(f\"   â€¢ Skip frames: Process every 2nd frame (2x faster)\")\n",
    "print(f\"   â€¢ Lower confidence: conf=0.6 â†’ 0.7 (fewer detections)\")\n",
    "\n",
    "print(\"\\nâœ… Exercise 3.2 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a120905-96c1-4cc9-99b9-82c08987ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ PART 4: KEY TAKEAWAYS & NEXT STEPS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ PART 4: KEY TAKEAWAYS & NEXT STEPS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "057964c8-f1e7-4084-a3a2-971adb5dd92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 4.1: Day 23 Summary\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š WHAT WE LEARNED TODAY:\n",
      "\n",
      "âœ… DeepSORT Theory & Architecture:\n",
      "   â€¢ Understood tracking vs detection\n",
      "   â€¢ Learned Kalman filter for motion prediction\n",
      "   â€¢ Learned appearance descriptor (ReID network)\n",
      "   â€¢ Hungarian algorithm for optimal matching\n",
      "   â€¢ Track lifecycle management\n",
      "\n",
      "âœ… Installation & Setup:\n",
      "   â€¢ Installed deep-sort-realtime package\n",
      "   â€¢ Configured tracking parameters\n",
      "   â€¢ Loaded YOLOv8n detection model\n",
      "   â€¢ Verified integration pipeline\n",
      "\n",
      "âœ… YOLO + DeepSORT Integration:\n",
      "   â€¢ Built format conversion function\n",
      "   â€¢ Integrated detection with tracking\n",
      "   â€¢ Tested on images and video\n",
      "   â€¢ Achieved persistent track IDs\n",
      "\n",
      "âœ… Visualization:\n",
      "   â€¢ Created color-coded bounding boxes\n",
      "   â€¢ Added trajectory visualization\n",
      "   â€¢ Built information panel\n",
      "   â€¢ Designed clear, readable overlays\n",
      "\n",
      "âœ… Performance Analysis:\n",
      "   â€¢ Benchmarked detection speed\n",
      "   â€¢ Measured tracking overhead\n",
      "   â€¢ Analyzed FPS performance\n",
      "   â€¢ Identified optimization strategies\n",
      "\n",
      "ğŸ“Š KEY METRICS TODAY:\n",
      "   â€¢ DeepSORT overhead: ~1-2ms per frame\n",
      "   â€¢ Tracking FPS: 20-30 FPS (CPU)\n",
      "   â€¢ Track ID persistence: Confirmed across frames\n",
      "   â€¢ Integration success: YOLO + DeepSORT working\n",
      "   â€¢ Visualization complete: Boxes + IDs + trajectories\n",
      "\n",
      "ğŸ’¡ KEY INSIGHTS:\n",
      "\n",
      "   1. DeepSORT adds minimal overhead (~5-10%)\n",
      "      â†’ Kalman filter is extremely efficient\n",
      "      â†’ Appearance descriptor is fast (MobileNet)\n",
      "\n",
      "   2. Track IDs persist reliably\n",
      "      â†’ Motion + appearance = robust tracking\n",
      "      â†’ Handles occlusions up to max_age frames\n",
      "\n",
      "   3. Real-time performance achievable\n",
      "      â†’ 20-30 FPS on CPU is usable\n",
      "      â†’ GPU would give 40-60 FPS\n",
      "\n",
      "   4. Visualization is crucial\n",
      "      â†’ Color-coding helps follow tracks\n",
      "      â†’ Trajectories show movement patterns\n",
      "      â†’ Performance metrics guide optimization\n",
      "\n",
      "   5. Integration is straightforward\n",
      "      â†’ Format conversion is simple\n",
      "      â†’ DeepSORT API is intuitive\n",
      "      â†’ Works with any detector (not just YOLO)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Exercise 4.1 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 4.1: DAY 23 SUMMARY\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 4.1: Day 23 Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“š WHAT WE LEARNED TODAY:\n",
    "\n",
    "âœ… DeepSORT Theory & Architecture:\n",
    "   â€¢ Understood tracking vs detection\n",
    "   â€¢ Learned Kalman filter for motion prediction\n",
    "   â€¢ Learned appearance descriptor (ReID network)\n",
    "   â€¢ Hungarian algorithm for optimal matching\n",
    "   â€¢ Track lifecycle management\n",
    "\n",
    "âœ… Installation & Setup:\n",
    "   â€¢ Installed deep-sort-realtime package\n",
    "   â€¢ Configured tracking parameters\n",
    "   â€¢ Loaded YOLOv8n detection model\n",
    "   â€¢ Verified integration pipeline\n",
    "\n",
    "âœ… YOLO + DeepSORT Integration:\n",
    "   â€¢ Built format conversion function\n",
    "   â€¢ Integrated detection with tracking\n",
    "   â€¢ Tested on images and video\n",
    "   â€¢ Achieved persistent track IDs\n",
    "\n",
    "âœ… Visualization:\n",
    "   â€¢ Created color-coded bounding boxes\n",
    "   â€¢ Added trajectory visualization\n",
    "   â€¢ Built information panel\n",
    "   â€¢ Designed clear, readable overlays\n",
    "\n",
    "âœ… Performance Analysis:\n",
    "   â€¢ Benchmarked detection speed\n",
    "   â€¢ Measured tracking overhead\n",
    "   â€¢ Analyzed FPS performance\n",
    "   â€¢ Identified optimization strategies\n",
    "\n",
    "ğŸ“Š KEY METRICS TODAY:\n",
    "   â€¢ DeepSORT overhead: ~1-2ms per frame\n",
    "   â€¢ Tracking FPS: 20-30 FPS (CPU)\n",
    "   â€¢ Track ID persistence: Confirmed across frames\n",
    "   â€¢ Integration success: YOLO + DeepSORT working\n",
    "   â€¢ Visualization complete: Boxes + IDs + trajectories\n",
    "\n",
    "ğŸ’¡ KEY INSIGHTS:\n",
    "\n",
    "   1. DeepSORT adds minimal overhead (~5-10%)\n",
    "      â†’ Kalman filter is extremely efficient\n",
    "      â†’ Appearance descriptor is fast (MobileNet)\n",
    "      \n",
    "   2. Track IDs persist reliably\n",
    "      â†’ Motion + appearance = robust tracking\n",
    "      â†’ Handles occlusions up to max_age frames\n",
    "      \n",
    "   3. Real-time performance achievable\n",
    "      â†’ 20-30 FPS on CPU is usable\n",
    "      â†’ GPU would give 40-60 FPS\n",
    "      \n",
    "   4. Visualization is crucial\n",
    "      â†’ Color-coding helps follow tracks\n",
    "      â†’ Trajectories show movement patterns\n",
    "      â†’ Performance metrics guide optimization\n",
    "      \n",
    "   5. Integration is straightforward\n",
    "      â†’ Format conversion is simple\n",
    "      â†’ DeepSORT API is intuitive\n",
    "      â†’ Works with any detector (not just YOLO)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Exercise 4.1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dc1cd53-dda8-4741-b2ec-418c60cbae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXERCISE 4.2: Tomorrow's Plan\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ DAY 24: TRACKING OPTIMIZATION (November 19, 2025)\n",
      "\n",
      "What we'll do:\n",
      "1. Handle occlusions robustly\n",
      "   â€¢ Tune max_age parameter\n",
      "   â€¢ Test with occluded scenarios\n",
      "   â€¢ Verify re-identification works\n",
      "\n",
      "2. Implement people counting\n",
      "   â€¢ Define entry/exit lines\n",
      "   â€¢ Count line crossings\n",
      "   â€¢ Track direction (in vs out)\n",
      "   â€¢ Maintain running totals\n",
      "\n",
      "3. Optimize tracking parameters\n",
      "   â€¢ Tune max_iou_distance\n",
      "   â€¢ Adjust n_init for faster confirmation\n",
      "   â€¢ Balance speed vs accuracy\n",
      "   â€¢ Test different scenarios\n",
      "\n",
      "4. Zone-based tracking\n",
      "   â€¢ Define restricted areas\n",
      "   â€¢ Count people per zone\n",
      "   â€¢ Detect zone violations\n",
      "   â€¢ Alert on capacity exceeded\n",
      "\n",
      "5. Performance optimization\n",
      "   â€¢ Resolution tuning\n",
      "   â€¢ Frame skipping strategy\n",
      "   â€¢ Batch processing experiments\n",
      "   â€¢ GPU acceleration testing\n",
      "\n",
      "Expected outcomes:\n",
      "   â€¢ Robust tracking through occlusions\n",
      "   â€¢ Accurate people counting (entry/exit)\n",
      "   â€¢ Optimized parameters for security use\n",
      "   â€¢ 30+ FPS on GPU, 20+ FPS on CPU\n",
      "   â€¢ Zone monitoring functional\n",
      "\n",
      "Tech Stack:\n",
      "   â€¢ DeepSORT (parameter tuning)\n",
      "   â€¢ OpenCV (line crossing detection)\n",
      "   â€¢ NumPy (geometry calculations)\n",
      "   â€¢ Matplotlib (visualization)\n",
      "\n",
      "Time estimate: 5-6 hours\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Exercise 4.2 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# EXERCISE 4.2: TOMORROW'S PLAN (DAY 24)\n",
    "# ==================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXERCISE 4.2: Tomorrow's Plan\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ DAY 24: TRACKING OPTIMIZATION (November 19, 2025)\n",
    "\n",
    "What we'll do:\n",
    "1. Handle occlusions robustly\n",
    "   â€¢ Tune max_age parameter\n",
    "   â€¢ Test with occluded scenarios\n",
    "   â€¢ Verify re-identification works\n",
    "\n",
    "2. Implement people counting\n",
    "   â€¢ Define entry/exit lines\n",
    "   â€¢ Count line crossings\n",
    "   â€¢ Track direction (in vs out)\n",
    "   â€¢ Maintain running totals\n",
    "\n",
    "3. Optimize tracking parameters\n",
    "   â€¢ Tune max_iou_distance\n",
    "   â€¢ Adjust n_init for faster confirmation\n",
    "   â€¢ Balance speed vs accuracy\n",
    "   â€¢ Test different scenarios\n",
    "\n",
    "4. Zone-based tracking\n",
    "   â€¢ Define restricted areas\n",
    "   â€¢ Count people per zone\n",
    "   â€¢ Detect zone violations\n",
    "   â€¢ Alert on capacity exceeded\n",
    "\n",
    "5. Performance optimization\n",
    "   â€¢ Resolution tuning\n",
    "   â€¢ Frame skipping strategy\n",
    "   â€¢ Batch processing experiments\n",
    "   â€¢ GPU acceleration testing\n",
    "\n",
    "Expected outcomes:\n",
    "   â€¢ Robust tracking through occlusions\n",
    "   â€¢ Accurate people counting (entry/exit)\n",
    "   â€¢ Optimized parameters for security use\n",
    "   â€¢ 30+ FPS on GPU, 20+ FPS on CPU\n",
    "   â€¢ Zone monitoring functional\n",
    "\n",
    "Tech Stack:\n",
    "   â€¢ DeepSORT (parameter tuning)\n",
    "   â€¢ OpenCV (line crossing detection)\n",
    "   â€¢ NumPy (geometry calculations)\n",
    "   â€¢ Matplotlib (visualization)\n",
    "\n",
    "Time estimate: 5-6 hours\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Exercise 4.2 Complete!\")\n",
    "print(\"=\" * 80\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc6f37af-8305-4675-a5fc-e4b475e8f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DAY 23 COMPLETE! âœ…\n",
      "================================================================================\n",
      "\n",
      "OBJECTIVES ACHIEVED:\n",
      "   âœ… Studied DeepSORT algorithm (Kalman, ReID, Hungarian)\n",
      "   âœ… Installed and configured DeepSORT\n",
      "   âœ… Integrated YOLO detection with DeepSORT tracking\n",
      "   âœ… Tracked people with unique, persistent IDs\n",
      "   âœ… Created advanced visualization functions\n",
      "   âœ… Benchmarked performance (20-30 FPS on CPU)\n",
      "   âœ… Built complete tracking pipeline\n",
      "   âœ… Tested on images and prepared for video\n",
      "\n",
      "ğŸ“Š KEY METRICS:\n",
      "   - DeepSORT overhead: ~1-2ms per frame\n",
      "   - Tracking FPS: 20-30 (CPU), 40-60 (GPU estimated)\n",
      "   - Track persistence: Up to 30 frames (1 sec @ 30fps)\n",
      "   - Visualization: Color-coded with trajectories\n",
      "   - Integration: YOLO + DeepSORT working seamlessly\n",
      "\n",
      "ğŸ’¡ KEY LEARNINGS:\n",
      "   - Tracking connects detections across time\n",
      "   - DeepSORT uses both motion and appearance\n",
      "   - Kalman filter predicts object movement\n",
      "   - ReID network recognizes same object after occlusion\n",
      "   - Minimal performance overhead (~5-10%)\n",
      "   - Track IDs enable people counting and behavior analysis\n",
      "   - Visualization crucial for debugging and demos\n",
      "\n",
      "ğŸ¯ TOMORROW (DAY 24):\n",
      "   - Optimize tracking parameters\n",
      "   - Implement people counting (entry/exit)\n",
      "   - Handle occlusions robustly\n",
      "   - Define zones for monitoring\n",
      "   - Test various scenarios\n",
      "   - Achieve 30+ FPS target\n",
      "\n",
      "ğŸ’¾ FILES CREATED TODAY:\n",
      "   - day23_deepsort_integration.ipynb (This notebook!)\n",
      "   - Functions: yolo_to_deepsort(), get_color(), draw_tracks(), draw_info_panel()\n",
      "   - Models downloaded: yolov8n.pt, DeepSORT embedder\n",
      "\n",
      "ğŸ”¥ PROGRESS UPDATE:\n",
      "   Week 4: 29% complete (2/7 days)\n",
      "   Overall: 13.7% complete (23/168 days)\n",
      "\n",
      "ğŸš€ MOMENTUM:\n",
      "   âœ… Week 1: Neural Networks (Complete)\n",
      "   âœ… Week 2: YOLO Detection (Complete - 75.1% mAP)\n",
      "   âœ… Week 3: Medical Classifier (Complete - 94.48%)\n",
      "   âœ… Day 22: Security System Planning (Complete)\n",
      "   âœ… Day 23: DeepSORT Integration (Complete - TODAY!)\n",
      "\n",
      "   Next: Tracking optimization and people counting! ğŸ¯\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DAY 23 COMPLETE! âœ…\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "OBJECTIVES ACHIEVED:\n",
    "   âœ… Studied DeepSORT algorithm (Kalman, ReID, Hungarian)\n",
    "   âœ… Installed and configured DeepSORT\n",
    "   âœ… Integrated YOLO detection with DeepSORT tracking\n",
    "   âœ… Tracked people with unique, persistent IDs\n",
    "   âœ… Created advanced visualization functions\n",
    "   âœ… Benchmarked performance (20-30 FPS on CPU)\n",
    "   âœ… Built complete tracking pipeline\n",
    "   âœ… Tested on images and prepared for video\n",
    "\n",
    "ğŸ“Š KEY METRICS:\n",
    "   - DeepSORT overhead: ~1-2ms per frame\n",
    "   - Tracking FPS: 20-30 (CPU), 40-60 (GPU estimated)\n",
    "   - Track persistence: Up to 30 frames (1 sec @ 30fps)\n",
    "   - Visualization: Color-coded with trajectories\n",
    "   - Integration: YOLO + DeepSORT working seamlessly\n",
    "\n",
    "ğŸ’¡ KEY LEARNINGS:\n",
    "   - Tracking connects detections across time\n",
    "   - DeepSORT uses both motion and appearance\n",
    "   - Kalman filter predicts object movement\n",
    "   - ReID network recognizes same object after occlusion\n",
    "   - Minimal performance overhead (~5-10%)\n",
    "   - Track IDs enable people counting and behavior analysis\n",
    "   - Visualization crucial for debugging and demos\n",
    "\n",
    "ğŸ¯ TOMORROW (DAY 24):\n",
    "   - Optimize tracking parameters\n",
    "   - Implement people counting (entry/exit)\n",
    "   - Handle occlusions robustly\n",
    "   - Define zones for monitoring\n",
    "   - Test various scenarios\n",
    "   - Achieve 30+ FPS target\n",
    "\n",
    "ğŸ’¾ FILES CREATED TODAY:\n",
    "   - day23_deepsort_integration.ipynb (This notebook!)\n",
    "   - Functions: yolo_to_deepsort(), get_color(), draw_tracks(), draw_info_panel()\n",
    "   - Models downloaded: yolov8n.pt, DeepSORT embedder\n",
    "\n",
    "ğŸ”¥ PROGRESS UPDATE:\n",
    "   Week 4: 29% complete (2/7 days)\n",
    "   Overall: 13.7% complete (23/168 days)\n",
    "   \n",
    "ğŸš€ MOMENTUM:\n",
    "   âœ… Week 1: Neural Networks (Complete)\n",
    "   âœ… Week 2: YOLO Detection (Complete - 75.1% mAP)\n",
    "   âœ… Week 3: Medical Classifier (Complete - 94.48%)\n",
    "   âœ… Day 22: Security System Planning (Complete)\n",
    "   âœ… Day 23: DeepSORT Integration (Complete - TODAY!)\n",
    "   \n",
    "   Next: Tracking optimization and people counting! ğŸ¯\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7535385-a9ef-46b6-b03f-fcc9ae4e3b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
